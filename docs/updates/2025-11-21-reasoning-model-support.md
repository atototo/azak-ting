# Reasoning Model Support êµ¬í˜„

**ì‘ì—… ì¼ì**: 2025-11-21
**ì‘ì—…ì**: Development Team
**ê´€ë ¨ ì´ìŠˆ**: Reasoning ëª¨ë¸(gpt-5-mini, o1, o3 ë“±) ì§€ì› ì¶”ê°€

---

## ğŸ“‹ ëª©ì°¨

1. [ë³€ê²½ ê°œìš”](#ë³€ê²½-ê°œìš”)
2. [AS-IS (ê¸°ì¡´ ìƒíƒœ)](#as-is-ê¸°ì¡´-ìƒíƒœ)
3. [ë³€ê²½ í•„ìš” ì‚¬ìœ ](#ë³€ê²½-í•„ìš”-ì‚¬ìœ )
4. [TO-BE (ë³€ê²½ í›„ ìƒíƒœ)](#to-be-ë³€ê²½-í›„-ìƒíƒœ)
5. [ë³€ê²½ ì‚¬í•­ ìƒì„¸](#ë³€ê²½-ì‚¬í•­-ìƒì„¸)
6. [í…ŒìŠ¤íŠ¸ ê²°ê³¼](#í…ŒìŠ¤íŠ¸-ê²°ê³¼)
7. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
8. [ì°¸ê³  ì‚¬í•­](#ì°¸ê³ -ì‚¬í•­)

---

## ë³€ê²½ ê°œìš”

OpenAIì˜ reasoning ëª¨ë¸(gpt-5-mini, o1, o3 ë“±)ì€ ì¼ë°˜ ëª¨ë¸ê³¼ ë‹¤ë¥¸ API ì‘ë‹µ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ëª¨ë¸ íƒ€ì… êµ¬ë¶„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³ , reasoning ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.

---

## AS-IS (ê¸°ì¡´ ìƒíƒœ)

### ë°ì´í„°ë² ì´ìŠ¤
```python
# backend/db/models/model.py
class Model(Base):
    __tablename__ = "models"

    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    provider = Column(String(50), nullable=False)
    model_identifier = Column(String(200), nullable=False)
    # âŒ model_type í•„ë“œ ì—†ìŒ
    is_active = Column(Boolean, default=True)
    description = Column(String(500))
    created_at = Column(DateTime, default=datetime.now)
```

### API
```python
# backend/api/models.py
class ModelCreate(BaseModel):
    name: str
    provider: str
    model_identifier: str
    # âŒ model_type í•„ë“œ ì—†ìŒ
    description: Optional[str] = None
```

### Predictor ë¡œì§
```python
# backend/llm/predictor.py
def _predict_with_model(self, client, model_name, provider, prompt, similar_count):
    # âŒ ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
    if provider == "openai":
        response = client.chat.completions.create(
            model=model_name,
            messages=[...],
            response_format={"type": "json_object"},  # ëª¨ë“  ëª¨ë¸ì— ì ìš©
            max_tokens=1000  # ê³ ì •ê°’
        )

    # âŒ content í•„ë“œë§Œ ì‚¬ìš©
    result_text = response.choices[0].message.content
```

### ë¬¸ì œì 
1. **ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•˜ê²Œ ì²˜ë¦¬**: normal vs reasoning êµ¬ë¶„ ì—†ìŒ
2. **Reasoning ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨**: `response_format` ë¯¸ì§€ì›ìœ¼ë¡œ API ì˜¤ë¥˜ ë°œìƒ
3. **ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨**: reasoning í•„ë“œ ì²˜ë¦¬ ë¶ˆê°€
4. **í† í° ë¶€ì¡±**: reasoning ëª¨ë¸ì˜ ê¸´ Chain of Thoughtë¥¼ ë‹´ê¸°ì— 1000 í† í° ë¶€ì¡±

---

## ë³€ê²½ í•„ìš” ì‚¬ìœ 

### 1. Reasoning ëª¨ë¸ì˜ íŠ¹ìˆ˜ì„±

OpenAIì˜ reasoning ëª¨ë¸(gpt-5-mini, o1, o3)ì€ ë‹¤ìŒê³¼ ê°™ì€ ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤:

#### API ìš”ì²­ ì°¨ì´
```python
# âŒ Normal ëª¨ë¸ - response_format ì§€ì›
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[...],
    response_format={"type": "json_object"},  # âœ… ì§€ì›
    max_tokens=1000
)

# âŒ Reasoning ëª¨ë¸ - response_format ë¯¸ì§€ì›
response = client.chat.completions.create(
    model="gpt-5-mini",
    messages=[...],
    response_format={"type": "json_object"},  # âŒ ì˜¤ë¥˜ ë°œìƒ!
    max_tokens=1000  # âŒ ë¶€ì¡±í•¨
)
```

#### API ì‘ë‹µ êµ¬ì¡° ì°¨ì´
```python
# Normal ëª¨ë¸ ì‘ë‹µ
ChatCompletionMessage(
    content='{"sentiment_direction": "positive", ...}',  # JSON ì§ì ‘ ë°˜í™˜
    reasoning=None  # ì—†ìŒ
)

# Reasoning ëª¨ë¸ ì‘ë‹µ
ChatCompletionMessage(
    content='{"sentiment_direction": "positive", ...}',  # JSON ë°˜í™˜
    reasoning='**Analyzing market impact**\n\n...',  # Chain of Thought ì¶”ê°€!
    reasoning_details=[...]  # ì•”í˜¸í™”ëœ ìƒì„¸ ë¶„ì„
)
```

### 2. í•„ìš”í•œ ë³€ê²½ ì‚¬í•­

1. **ëª¨ë¸ íƒ€ì… êµ¬ë¶„**: `normal` vs `reasoning` íƒ€ì… ì¶”ê°€
2. **API í˜¸ì¶œ ë¶„ê¸°**: reasoning ëª¨ë¸ì€ `response_format` ì œê±°
3. **í† í° ì œí•œ ì™„í™”**: 1000 â†’ 4000 í† í°ìœ¼ë¡œ ì¦ê°€
4. **ì‘ë‹µ íŒŒì‹± ê°œì„ **: content ë¹„ì–´ìˆì„ ë•Œ reasoning í•„ë“œ í™œìš©
5. **JSON ì¶”ì¶œ ê°•í™”**: Chain of Thoughtì—ì„œ JSON ê°ì²´ ì¶”ì¶œ

---

## TO-BE (ë³€ê²½ í›„ ìƒíƒœ)

### ë°ì´í„°ë² ì´ìŠ¤
```python
# backend/db/models/model.py
class Model(Base):
    __tablename__ = "models"

    id = Column(Integer, primary_key=True)
    name = Column(String(100), nullable=False, unique=True)
    provider = Column(String(50), nullable=False)
    model_identifier = Column(String(200), nullable=False)
    model_type = Column(
        SQLEnum("normal", "reasoning", name="model_type_enum"),
        default="normal",
        nullable=False
    )  # âœ… ì¶”ê°€ë¨
    is_active = Column(Boolean, default=True)
    description = Column(String(500))
    created_at = Column(DateTime, default=datetime.now)
```

### API
```python
# backend/api/models.py
class ModelCreate(BaseModel):
    name: str
    provider: str
    model_identifier: str
    model_type: Literal["normal", "reasoning"] = Field(
        default="normal",
        description="ëª¨ë¸ íƒ€ì… (normal: ì¼ë°˜, reasoning: ì¶”ë¡ í˜•)"
    )  # âœ… ì¶”ê°€ë¨
    description: Optional[str] = None

class ModelUpdate(BaseModel):
    name: Optional[str] = None
    provider: Optional[str] = None
    model_identifier: Optional[str] = None
    model_type: Optional[Literal["normal", "reasoning"]] = None  # âœ… ì¶”ê°€ë¨
    description: Optional[str] = None
    is_active: Optional[bool] = None
```

### Predictor ë¡œì§
```python
# backend/llm/predictor.py
def _load_active_models(self):
    """í™œì„± ëª¨ë¸ ë¡œë“œ (model_type í¬í•¨)"""
    for model in models:
        result[model.id] = {
            "name": model.name,
            "provider": model.provider,
            "model_identifier": model.model_identifier,
            "model_type": model.model_type,  # âœ… ì¶”ê°€ë¨
            "client": client,
        }

def _predict_with_model(
    self,
    client: OpenAI,
    model_name: str,
    provider: str,
    prompt: str,
    similar_count: int,
    model_type: str = "normal",  # âœ… íŒŒë¼ë¯¸í„° ì¶”ê°€
) -> Dict[str, Any]:
    # âœ… ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³€ê²½
    is_reasoning_model = model_type == "reasoning"

    if is_reasoning_model:
        system_content = (
            "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ë‰´ìŠ¤ ë¶„ì„ì„ í†µí•´ ì£¼ê°€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. "
            "ì‚¬ê³  ê³¼ì •ì„ ê±°ì¹œ í›„, ë°˜ë“œì‹œ ë§ˆì§€ë§‰ì— JSON í˜•ì‹ì˜ ê²°ê³¼ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
        )
    else:
        system_content = (
            "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ë‰´ìŠ¤ ë¶„ì„ì„ í†µí•´ ì£¼ê°€ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. "
            "ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”."
        )

    # âœ… API í˜¸ì¶œ ë¶„ê¸°
    if provider == "openrouter":
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=4000 if is_reasoning_model else 1000,  # âœ… ë™ì  í† í°
        )
    else:  # openai
        api_params = {
            "model": model_name,
            "messages": [
                {"role": "system", "content": system_content},
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.3,
            "max_tokens": 4000 if is_reasoning_model else 1000,  # âœ… ë™ì  í† í°
        }

        # âœ… ì¼ë°˜ ëª¨ë¸ë§Œ response_format ì‚¬ìš©
        if not is_reasoning_model:
            api_params["response_format"] = {"type": "json_object"}

        response = client.chat.completions.create(**api_params)

    # âœ… ì‘ë‹µ íŒŒì‹± ê°œì„ 
    message = response.choices[0].message
    result_text = message.content

    # contentê°€ ë¹„ì–´ìˆìœ¼ë©´ reasoning í•„ë“œ í™•ì¸
    if not result_text and hasattr(message, 'reasoning') and message.reasoning:
        result_text = message.reasoning
        logger.info(f"ğŸ’¡ content ë¹„ì–´ìˆìŒ, reasoning í•„ë“œ ì‚¬ìš©")

    # âœ… JSON ì¶”ì¶œ ê°•í™” (```json``` ì½”ë“œ ë¸”ë¡ ì œê±°)
    if "```json" in result_text:
        match = re.search(r"```json\s*(\{.*?\})\s*```", result_text, re.DOTALL)
        if match:
            result_text = match.group(1)

    # âœ… ë§ˆì§€ë§‰ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„
    if is_reasoning_model and not result_text.strip().startswith('{'):
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}(?=[^{}]*$)', result_text, re.DOTALL)
        if json_match:
            result_text = json_match.group(0)
```

### ê°œì„  ì‚¬í•­

1. âœ… **ëª¨ë¸ íƒ€ì… ì‹œìŠ¤í…œ**: `normal` / `reasoning` êµ¬ë¶„
2. âœ… **API í˜¸ì¶œ ìµœì í™”**: ëª¨ë¸ë³„ ì ì ˆí•œ íŒŒë¼ë¯¸í„° ì‚¬ìš©
3. âœ… **í† í° ì œí•œ ì™„í™”**: 4000 í† í°ìœ¼ë¡œ Chain of Thought ì™„ì „ ìˆ˜ìš©
4. âœ… **ì‘ë‹µ íŒŒì‹± ê°•í™”**: reasoning í•„ë“œ ë° JSON ì¶”ì¶œ ë¡œì§
5. âœ… **ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ëª¨ë¸ë³„ íŠ¹ì„±ì— ë§ëŠ” ì§€ì‹œì‚¬í•­

---

## ë³€ê²½ ì‚¬í•­ ìƒì„¸

### 1. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜

#### PostgreSQL ENUM íƒ€ì… ìƒì„±
```sql
-- /tmp/add_model_type_column.py
DO $$ BEGIN
    CREATE TYPE model_type_enum AS ENUM ('normal', 'reasoning');
EXCEPTION
    WHEN duplicate_object THEN null;
END $$;
```

#### ì»¬ëŸ¼ ì¶”ê°€
```sql
DO $$ BEGIN
    ALTER TABLE models
    ADD COLUMN model_type model_type_enum DEFAULT 'normal' NOT NULL;
EXCEPTION
    WHEN duplicate_column THEN
        RAISE NOTICE 'column model_type already exists';
END $$;
```

**ì‹¤í–‰ ë°©ë²•**:
```bash
uv run python /tmp/add_model_type_column.py
```

### 2. API ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸

**íŒŒì¼**: `backend/api/models.py`

#### ë³€ê²½ ë‚´ìš©
```python
from typing import Literal

class ModelCreate(BaseModel):
    model_type: Literal["normal", "reasoning"] = Field(
        default="normal",
        description="ëª¨ë¸ íƒ€ì… (normal: ì¼ë°˜, reasoning: ì¶”ë¡ í˜•)"
    )

class ModelUpdate(BaseModel):
    model_type: Optional[Literal["normal", "reasoning"]] = None

class ModelResponse(BaseModel):
    model_type: str
```

#### ëª¨ë¸ ìƒì„± ë¡œì§ ìˆ˜ì •
```python
@router.post("/models", response_model=ModelResponse, status_code=201)
async def create_model(model: ModelCreate):
    new_model = Model(
        name=model.name,
        provider=model.provider,
        model_identifier=model.model_identifier,
        model_type=model.model_type,  # âœ… ì¶”ê°€
        description=model.description,
        is_active=True,
    )
    # ... ë‚˜ë¨¸ì§€ ë¡œì§
```

### 3. Predictor ë¡œì§ ì—…ë°ì´íŠ¸

**íŒŒì¼**: `backend/llm/predictor.py`

#### ì£¼ìš” ë³€ê²½ í•¨ìˆ˜

##### `_load_active_models()` - ëª¨ë¸ ë¡œë“œ ì‹œ model_type í¬í•¨
```python
def _load_active_models(self) -> Dict[int, Dict[str, Any]]:
    result[model.id] = {
        "name": model.name,
        "provider": model.provider,
        "model_identifier": model.model_identifier,
        "model_type": model.model_type,  # âœ… ì¶”ê°€
        "client": client,
        "description": model.description,
    }
    logger.info(
        f"  ğŸ“Š Model loaded: {model.name} "
        f"({model.provider}/{model.model_identifier}, "
        f"type={model.model_type})"  # âœ… ë¡œê¹… ì¶”ê°€
    )
```

##### `_predict_with_model()` - model_type íŒŒë¼ë¯¸í„° ì¶”ê°€
```python
def _predict_with_model(
    self,
    client: OpenAI,
    model_name: str,
    provider: str,
    prompt: str,
    similar_count: int,
    model_type: str = "normal",  # âœ… ì¶”ê°€
) -> Dict[str, Any]:
    # ... êµ¬í˜„
```

##### ëª¨ë“  í˜¸ì¶œë¶€ ì—…ë°ì´íŠ¸
```python
# predict_all_models()
model_info = self.models[model_id]
result = self._predict_with_model(
    client=model_info["client"],
    model_name=model_info["model_identifier"],
    provider=model_info["provider"],
    prompt=prompt,
    similar_count=similar_count,
    model_type=model_info["model_type"],  # âœ… ì¶”ê°€
)

# dual_predict() - A/B í…ŒìŠ¤íŠ¸
self.model_a_type = self.models[model_a_id]["model_type"]  # âœ… ì¶”ê°€
self.model_b_type = self.models[model_b_id]["model_type"]  # âœ… ì¶”ê°€

# A/B ì˜ˆì¸¡ í˜¸ì¶œ ì‹œ
result_a = self._predict_with_model(..., model_type=self.model_a_type)
result_b = self._predict_with_model(..., model_type=self.model_b_type)
```

### 4. ëª¨ë¸ ì¬ë“±ë¡ ìŠ¤í¬ë¦½íŠ¸

**íŒŒì¼**: `/tmp/register_gpt5mini_reasoning.py`

```python
"""gpt-5-minië¥¼ reasoning íƒ€ì…ìœ¼ë¡œ ë“±ë¡"""
import sys
sys.path.insert(0, '/Users/young/ai-work/craveny')

from backend.db.session import SessionLocal
from backend.db.models.prediction import Prediction
from backend.db.models.model import Model
import requests
import time

db = SessionLocal()

try:
    # 1. ê¸°ì¡´ ëª¨ë¸ ì‚­ì œ
    model = db.query(Model).filter(Model.name.like('%gpt-5-mini%')).first()

    if model:
        # ì˜ˆì¸¡ ë°ì´í„° ì‚­ì œ
        pred_count = db.query(Prediction).filter(
            Prediction.model_id == model.id
        ).delete()
        db.commit()

        # ëª¨ë¸ ì‚­ì œ
        db.delete(model)
        db.commit()
        print(f"ğŸ—‘ï¸  ëª¨ë¸ ì‚­ì œ ì™„ë£Œ: {pred_count}ê±´ ì˜ˆì¸¡ í¬í•¨")

    time.sleep(1)

    # 2. reasoning íƒ€ì…ìœ¼ë¡œ ì¬ë“±ë¡
    response = requests.post(
        'http://127.0.0.1:8000/api/models',
        json={
            "name": "gpt-5-mini",
            "provider": "openrouter",
            "model_identifier": "openai/gpt-5-mini",
            "model_type": "reasoning",  # âœ… reasoning íƒ€ì…
            "description": "OpenAI GPT-5 Mini (Reasoning Model)"
        }
    )

    if response.status_code in [200, 201]:
        print("âœ… ëª¨ë¸ ë“±ë¡ ì™„ë£Œ")
    else:
        print(f"âŒ ë“±ë¡ ì‹¤íŒ¨: {response.status_code}")

finally:
    db.close()
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼

### 1. ëª¨ë¸ ë“±ë¡ í…ŒìŠ¤íŠ¸

```bash
âœ… ëª¨ë¸ ì¶”ê°€ ì™„ë£Œ: gpt-5-mini (ID: 15)
ğŸ”„ Predictor ëª¨ë¸ ëª©ë¡ ì¬ë¡œë“œ ì™„ë£Œ
ğŸ”„ ë°±ê·¸ë¼ìš´ë“œ ì˜ˆì¸¡ ìƒì„± ì‹œì‘: model=gpt-5-mini, total=20, scheduled=20
```

### 2. ì˜ˆì¸¡ ìƒì„± í…ŒìŠ¤íŠ¸

**ë‰´ìŠ¤ ID 6797**ì— ëŒ€í•´ 4ê°œ ëª¨ë¸ ëª¨ë‘ ì˜ˆì¸¡ ì™„ë£Œ:

| ëª¨ë¸ | ID | íƒ€ì… | ìƒíƒœ | sentiment_score | impact_level |
|------|----|----|------|-----------------|--------------|
| gpt-5-mini | 15 | reasoning | âœ… | 0.35 | medium |
| Qwen3 Max | 5 | normal | âœ… | 0.65 | medium |
| DeepSeek V3.2 | 2 | normal | âœ… | 0.65 | medium |
| GPT-4o | 1 | normal | âœ… | 0.60 | medium |

### 3. Reasoning ëª¨ë¸ ë°ì´í„° ê²€ì¦

```python
# gpt-5-mini (reasoning) ì˜ˆì¸¡ ê²°ê³¼
{
    "id": 7582,
    "model_id": 15,
    "sentiment_direction": "positive",
    "sentiment_score": 0.35,
    "impact_level": "medium",
    "relevance_score": 0.6,
    "urgency_level": "notable",
    "reasoning": "í•œí™”ìì‚°ìš´ìš©ì˜ TDF 2040Â·2045 ë¹ˆí‹°ì§€ ìˆ˜ìµë¥  1ìœ„ ë°œí‘œëŠ”...",  # 501 chars
    "impact_analysis": {
        "business_impact": "ìì‚°ìš´ìš© ë¶€ë¬¸ì— ì§ì ‘ì  ê¸ì •...",
        "market_sentiment_impact": "íˆ¬ìì‹¬ë¦¬ ê°œì„  íš¨ê³¼ëŠ”...",
        "competitive_impact": "TDF ì‹œì¥ ë‚´ ìˆœìœ„Â·ë¸Œëœë“œ...",
        "regulatory_impact": "ê·œì œÂ·ì •ì±… ì¸¡ë©´ì˜ ì¦‰ê°ì  ë³€í™” ì—†ìŒ..."
    },
    "pattern_analysis": {
        "note": "ìœ ì‚¬ ì‹œì¥ ë™í–¥ ë°ì´í„° ì—†ìŒ",
        "avg_1d": null,
        "avg_3d": null,
        ...
    }
}
```

âœ… **ëª¨ë“  í•„ë“œê°€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë¨**

### 4. API ì‘ë‹µ êµ¬ì¡° í™•ì¸

```python
# Reasoning ëª¨ë¸ ì‘ë‹µ (ë¡œê·¸ ë°œì·Œ)
ChatCompletionMessage(
    content='{\n  "sentiment_direction": "positive",\n  "sentiment_score": 0.35, ...}',
    reasoning='**Analyzing market impact**\n\nI notice the market info...',
    reasoning_details=[
        {
            'format': 'openai-responses-v1',
            'index': 0,
            'type': 'reasoning.summary',
            'summary': '...'
        },
        {
            'id': 'rs_...',
            'format': 'openai-responses-v1',
            'type': 'reasoning.encrypted',
            'data': 'gAAAAABp...'
        }
    ]
)
```

âœ… **reasoning í•„ë“œ ë° reasoning_details ì •ìƒ ìˆ˜ì‹ **

---

## ì‚¬ìš© ë°©ë²•

### 1. ìƒˆ Reasoning ëª¨ë¸ ë“±ë¡

#### APIë¥¼ í†µí•œ ë“±ë¡
```bash
curl -X POST http://localhost:8000/api/models \
  -H "Content-Type: application/json" \
  -d '{
    "name": "o1-preview",
    "provider": "openai",
    "model_identifier": "o1-preview",
    "model_type": "reasoning",
    "description": "OpenAI o1 Preview (Reasoning Model)"
  }'
```

#### Python ìŠ¤í¬ë¦½íŠ¸ë¡œ ë“±ë¡
```python
import requests

response = requests.post(
    'http://127.0.0.1:8000/api/models',
    json={
        "name": "o1-preview",
        "provider": "openai",
        "model_identifier": "o1-preview",
        "model_type": "reasoning",
        "description": "OpenAI o1 Preview"
    }
)

print(response.json())
```

### 2. ê¸°ì¡´ ëª¨ë¸ íƒ€ì… ë³€ê²½

```bash
curl -X PUT http://localhost:8000/api/models/15 \
  -H "Content-Type: application/json" \
  -d '{
    "model_type": "reasoning"
  }'
```

### 3. ë°±ì—”ë“œ ì¬ì‹œì‘ (í•„ìˆ˜!)

ëª¨ë¸ ë“±ë¡ í›„ ë°˜ë“œì‹œ ë°±ì—”ë“œë¥¼ ì¬ì‹œì‘í•´ì•¼ Predictorê°€ ìƒˆ ëª¨ë¸ì„ ì¸ì‹í•©ë‹ˆë‹¤:

```bash
pm2 restart azak-backend
```

### 4. ì˜ˆì¸¡ ìƒì„± í™•ì¸

```bash
# ë¡œê·¸ í™•ì¸
pm2 logs azak-backend --lines 100

# ì˜ˆì¸¡ ë°ì´í„° ì¡°íšŒ
curl http://localhost:8000/api/predictions?news_id=6797
```

---

## ì°¸ê³  ì‚¬í•­

### 1. Reasoning ëª¨ë¸ ëª©ë¡

í˜„ì¬ ì§€ì› ê°€ëŠ¥í•œ reasoning ëª¨ë¸:

| ëª¨ë¸ëª… | Provider | Model Identifier | íŠ¹ì§• |
|--------|----------|-----------------|------|
| gpt-5-mini | openrouter | openai/gpt-5-mini | GPT-5 Mini (ê²½ëŸ‰ ì¶”ë¡ ) |
| o1-preview | openai | o1-preview | OpenAI o1 Preview |
| o1-mini | openai | o1-mini | OpenAI o1 Mini |
| o3-mini | openai | o3-mini | OpenAI o3 Mini |

### 2. Reasoning vs Normal ëª¨ë¸ ì°¨ì´

| êµ¬ë¶„ | Normal ëª¨ë¸ | Reasoning ëª¨ë¸ |
|------|------------|---------------|
| **response_format** | âœ… ì§€ì› (`{"type": "json_object"}`) | âŒ ë¯¸ì§€ì› (ì˜¤ë¥˜ ë°œìƒ) |
| **max_tokens** | 1000 (ì¶©ë¶„) | 4000 (Chain of Thought í•„ìš”) |
| **ì‘ë‹µ í•„ë“œ** | `content`ë§Œ ì‚¬ìš© | `content` + `reasoning` + `reasoning_details` |
| **JSON ì¶”ì¶œ** | ì§ì ‘ ë°˜í™˜ | Chain of Thoughtì—ì„œ ì¶”ì¶œ í•„ìš” |
| **ë¹„ìš©** | ìƒëŒ€ì ìœ¼ë¡œ ì €ë ´ | ìƒëŒ€ì ìœ¼ë¡œ ë¹„ìŒˆ (í† í° ë§ì´ ì‚¬ìš©) |

### 3. ì£¼ì˜ ì‚¬í•­

#### âš ï¸ ë°±ì—”ë“œ ì¬ì‹œì‘ í•„ìˆ˜
- ëª¨ë¸ ë“±ë¡/ìˆ˜ì • í›„ **ë°˜ë“œì‹œ** ë°±ì—”ë“œ ì¬ì‹œì‘ í•„ìš”
- PredictorëŠ” ì´ˆê¸°í™” ì‹œì ì—ë§Œ ëª¨ë¸ ëª©ë¡ ë¡œë“œ

#### âš ï¸ í† í° ë¹„ìš© ì¦ê°€
- Reasoning ëª¨ë¸ì€ 4000 í† í°ê¹Œì§€ ì‚¬ìš© (ì¼ë°˜ ëª¨ë¸ì˜ 4ë°°)
- Chain of Thought ìƒì„±ìœ¼ë¡œ ì¶”ê°€ ë¹„ìš© ë°œìƒ
- ë¹„ìš© ëª¨ë‹ˆí„°ë§ ê¶Œì¥

#### âš ï¸ ì‘ë‹µ ì‹œê°„ ì¦ê°€
- Reasoning ëª¨ë¸ì€ ì¶”ë¡  ê³¼ì •ì´ ê¸¸ì–´ ì‘ë‹µ ì‹œê°„ ì¦ê°€
- í‰ê·  ì‘ë‹µ ì‹œê°„: normal 3~5ì´ˆ, reasoning 10~20ì´ˆ

### 4. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

#### ë¬¸ì œ: "model_type ì»¬ëŸ¼ ì—†ìŒ" ì˜¤ë¥˜
```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
uv run python /tmp/add_model_type_column.py
```

#### ë¬¸ì œ: Reasoning ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨
```bash
# 1. ë°±ì—”ë“œ ì¬ì‹œì‘
pm2 restart azak-backend

# 2. ëª¨ë¸ ëª©ë¡ í™•ì¸
curl http://localhost:8000/api/models

# 3. ë¡œê·¸ í™•ì¸
pm2 logs azak-backend --lines 50
```

#### ë¬¸ì œ: JSON íŒŒì‹± ì‹¤íŒ¨
- **ì›ì¸**: max_tokens ë¶€ì¡±ìœ¼ë¡œ ì‘ë‹µ ì˜ë¦¼
- **í•´ê²°**: ì½”ë“œì—ì„œ ì´ë¯¸ 4000 í† í°ìœ¼ë¡œ ì„¤ì •ë¨ (í™•ì¸ í•„ìš”)

---

## ê´€ë ¨ íŒŒì¼

### ìˆ˜ì •ëœ íŒŒì¼
- `backend/db/models/model.py` - Model í´ë˜ìŠ¤ì— model_type ì»¬ëŸ¼ ì¶”ê°€
- `backend/api/models.py` - API ìŠ¤í‚¤ë§ˆì— model_type í•„ë“œ ì¶”ê°€
- `backend/llm/predictor.py` - Predictor ë¡œì§ì— reasoning ëª¨ë¸ ì²˜ë¦¬ ì¶”ê°€

### ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸
- `/tmp/add_model_type_column.py` - DB ë§ˆì´ê·¸ë ˆì´ì…˜
- `/tmp/register_gpt5mini_reasoning.py` - gpt-5-mini ì¬ë“±ë¡ ìŠ¤í¬ë¦½íŠ¸

### í…ŒìŠ¤íŠ¸ ë°ì´í„°
- ë‰´ìŠ¤ ID: 6797
- ì˜ˆì¸¡ ID: 7579 (GPT-4o), 7580 (DeepSeek), 7581 (Qwen3), 7582 (gpt-5-mini)

---

## ë³€ê²½ ì´ë ¥

| ë‚ ì§œ | ë²„ì „ | ë³€ê²½ ë‚´ìš© |
|------|------|----------|
| 2025-11-21 | 1.0.0 | Reasoning ëª¨ë¸ ì§€ì› ì¶”ê°€ (model_type enum, API ë¶„ê¸°, í† í° ì¦ê°€) |

---

**ì‘ì„±ì¼**: 2025-11-21
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-11-21
**ì‘ì„±ì**: Development Team
