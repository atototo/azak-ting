"""
Stock Analysis Service

ì˜ˆì¸¡ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ì¢…í•© íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì„œë¹„ìŠ¤
"""
import logging
import json
import re
import asyncio
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from sqlalchemy import func

from backend.db.models.prediction import Prediction
from backend.db.models.stock_analysis import StockAnalysisSummary
from backend.db.models.stock import StockPrice, Stock
from backend.db.models.model import Model
from backend.db.models.ab_test_config import ABTestConfig
from backend.db.models.market_data import StockCurrentPrice, InvestorTrading
from backend.db.models.financial import FinancialRatio, ProductInfo
from backend.db.models.news import NewsArticle
from backend.llm.investment_report import get_report_generator
from backend.utils.stock_mapping import get_stock_mapper
from backend.utils.market_time import (
    get_market_phase,
    get_ttl_hours,
    get_price_threshold,
    get_direction_threshold
)
from backend.crawlers.kis_client import KISClient
from backend.crawlers.kis_daily_crawler import KISDailyCrawler
from backend.services.kis_data_service import save_product_info, save_financial_ratios


logger = logging.getLogger(__name__)


async def trigger_initial_analysis(stock_code: str, db: Session):
    """
    ì‹ ê·œ ì¢…ëª© ë“±ë¡ ì‹œ ì¦‰ì‹œ ë¶„ì„ ì‹¤í–‰

    1. KIS APIë¡œ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (1íšŒë§Œ)
    2. DBì— ì €ì¥
    3. ì´ˆê¸° ë¦¬í¬íŠ¸ ìƒì„±

    Args:
        stock_code: ì¢…ëª©ì½”ë“œ
        db: DB ì„¸ì…˜

    Raises:
        Exception: ì¹˜ëª…ì  ì˜¤ë¥˜ ì‹œ (ë¡œê·¸ë§Œ ê¸°ë¡, re-raise ì•ˆ í•¨)
    """
    logger.info(f"ğŸš€ Triggering initial analysis for {stock_code}")

    try:
        client = KISClient()

        # KIS API í˜¸ì¶œ (ì´ˆê¸° 1íšŒë§Œ)
        tasks = [
            client.get_current_price(stock_code),
            client.get_product_info(stock_code),
            client.get_financial_ratios(stock_code, div_cls_code="0")
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        current_price_data = results[0] if not isinstance(results[0], Exception) else None
        product_info_data = results[1] if not isinstance(results[1], Exception) else None
        financial_ratios_data = results[2] if not isinstance(results[2], Exception) else None

        # DB ì €ì¥ (ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬)
        if product_info_data:
            save_product_info(db, stock_code, product_info_data)
            logger.info(f"âœ… Saved product info for {stock_code}")

        if financial_ratios_data:
            save_financial_ratios(db, stock_code, financial_ratios_data)
            logger.info(f"âœ… Saved financial ratios for {stock_code}")

        # ê³¼ê±° ì£¼ê°€ ë°ì´í„° ìˆ˜ì§‘ (ìµœê·¼ 100ì¼)
        try:
            crawler = KISDailyCrawler()
            start_date = datetime.now() - timedelta(days=100)
            df = await crawler.fetch_daily_prices(stock_code, start_date)

            if df is not None and not df.empty:
                saved_count = crawler.save_to_db(stock_code, df, db)
                logger.info(f"âœ… Saved {saved_count} historical price records for {stock_code}")
            else:
                logger.warning(f"âš ï¸ No historical price data available for {stock_code}")
        except Exception as e:
            logger.error(f"âŒ Historical price collection failed for {stock_code}: {e}")
            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

        # ì´ˆê¸° ë¦¬í¬íŠ¸ ìƒì„± (í†µí•© í•¨ìˆ˜ ì‚¬ìš©)
        reports = await generate_unified_stock_report(stock_code, db)

        if reports:
            logger.info(f"âœ… Initial DB-based reports generated for {stock_code} ({len(reports)} models)")
        else:
            # DB ë°ì´í„°ë„ ì—†ì–´ì„œ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ - Placeholder ìƒì„±
            logger.warning(f"âš ï¸ No data available for {stock_code}, creating placeholder report")
            await create_placeholder_report(
                stock_code, db,
                error_msg="ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘ - KIS API ë°ì´í„° ìˆ˜ì§‘ í›„ ì¬ì‹œë„ ì˜ˆì •"
            )

    except Exception as e:
        logger.error(f"âŒ Initial analysis failed for {stock_code}: {e}")
        # Placeholder ë¦¬í¬íŠ¸ ìƒì„± ì‹œë„
        try:
            await create_placeholder_report(stock_code, db, error_msg=str(e))
        except Exception as e2:
            logger.error(f"âŒ Placeholder report failed for {stock_code}: {e2}")


async def generate_unified_stock_report(
    stock_code: str,
    db: Session,
    force_update: bool = False
) -> List[StockAnalysisSummary]:
    """
    í†µí•© ì¢…ëª© ë¦¬í¬íŠ¸ ìƒì„± - DB + Prediction í†µí•©, ì „ì²´ ëª¨ë¸ ì§€ì›

    ëª¨ë“  ê°€ìš© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ë™ì ìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
    - ì‹ ê·œ ì¢…ëª© (DBë§Œ): DB ë°ì´í„°ë¡œ ì´ˆê¸° ë¦¬í¬íŠ¸ ìƒì„±
    - ê¸°ì¡´ ì¢…ëª© (DB + Prediction): ëª¨ë“  ë°ì´í„°ë¥¼ í™œìš©í•œ ê³ í’ˆì§ˆ ë¦¬í¬íŠ¸

    í”„ë¡œì„¸ìŠ¤:
    1. í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• (DB + Prediction)
    2. ë°ì´í„° ê°€ìš©ì„± í™•ì¸
    3. í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
    4. ëª¨ë“  í™œì„± ëª¨ë¸ì— ëŒ€í•´ ë¦¬í¬íŠ¸ ìƒì„± (ë³‘ë ¬)
    5. ë©”íƒ€ë°ì´í„° í¬í•¨ (data_sources_used, limitations, confidence_level)

    Args:
        stock_code: ì¢…ëª©ì½”ë“œ
        db: DB ì„¸ì…˜
        force_update: ê°•ì œ ì—…ë°ì´íŠ¸ (ê¸°ë³¸ê°’: False)

    Returns:
        ìƒì„±ëœ StockAnalysisSummary ë¦¬ìŠ¤íŠ¸ (ê° ëª¨ë¸ë³„ 1ê°œì”©)
    """
    logger.info(f"ğŸ“Š Generating unified stock report for {stock_code}")

    try:
        # 1. í†µí•© ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶• (DB + Prediction)
        context = await build_unified_context(stock_code, db)

        # 2. ë°ì´í„° ê°€ìš©ì„± í™•ì¸
        data_sources = context.get("data_sources", {})
        available_count = sum(1 for v in data_sources.values() if v)

        if available_count == 0:
            logger.warning(f"No data available for {stock_code}")
            return []

        logger.info(f"âœ… Data sources available: {available_count}/8")

        # 3. í†µí•© í”„ë¡¬í”„íŠ¸ ìƒì„±
        from backend.llm.investment_report import build_unified_prompt
        prompt = build_unified_prompt(context)

        # 4. ëª¨ë“  í™œì„± ëª¨ë¸ ì¡°íšŒ
        active_models: List[Model] = db.query(Model).filter(Model.is_active == True).all()

        if not active_models:
            logger.error("No active LLM model found")
            return []

        logger.info(f"ğŸ“‹ Generating reports for {len(active_models)} active models")

        # 5. Prediction í†µê³„ (ìˆìœ¼ë©´ í¬í•¨)
        predictions_data = context.get("predictions")
        if predictions_data:
            stats = predictions_data.get("statistics", {})
            total_predictions = stats.get("total", 0)
            up_count = stats.get("positive", 0)
            down_count = stats.get("negative", 0)
            hold_count = stats.get("neutral", 0)
            logger.info(f"ğŸ“ˆ Predictions: {total_predictions}ê±´ (positive: {up_count}, negative: {down_count}, neutral: {hold_count})")
        else:
            total_predictions = 0
            up_count = 0
            down_count = 0
            hold_count = 0
            logger.info("â„¹ï¸ No predictions data")

        # 6. ê° ëª¨ë¸ë³„ë¡œ ë¦¬í¬íŠ¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
        from backend.llm.investment_report import get_report_generator
        generator = get_report_generator()

        # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
        async def generate_for_single_model(model: Model):
            """ë‹¨ì¼ ëª¨ë¸ì— ëŒ€í•œ ë¦¬í¬íŠ¸ ìƒì„± (ë¹„ë™ê¸°)"""
            try:
                logger.info(f"  ğŸ”„ Generating report with {model.name} ({model.provider})")

                # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                client = generator._create_client(model.provider)

                messages = [
                    {
                        "role": "system",
                        "content": "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ ì‹œì¥ì˜ ë² í…Œë‘ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. DB ë°ì´í„°ì™€ AI ì˜ˆì¸¡ì„ í†µí•©í•˜ì—¬ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
                    },
                    {"role": "user", "content": prompt},
                ]

                kwargs = {
                    "model": model.model_identifier,
                    "messages": messages,
                    "temperature": 0.4,
                    "max_tokens": 4000 if model.model_type == "reasoning" else 2000,
                }

                # ì¼ë°˜ ëª¨ë¸ë§Œ response_format ì‚¬ìš© (reasoning ëª¨ë¸ ì œì™¸)
                if model.provider != "openrouter" and model.model_type != "reasoning":
                    kwargs["response_format"] = {"type": "json_object"}

                # ë™ê¸° APIë¥¼ asyncio.to_threadë¡œ ë¹„ë™ê¸° ì²˜ë¦¬
                response = await asyncio.to_thread(
                    client.chat.completions.create, **kwargs
                )
                result_text = response.choices[0].message.content

                # OpenRouter JSON ì¶”ì¶œ
                if model.provider == "openrouter":
                    result_text = _extract_openrouter_json(result_text)

                # ë””ë²„ê¹…: ì‘ë‹µ ë‚´ìš© ê²€ì¦ ë° ë¡œê¹…
                if not result_text or not result_text.strip():
                    logger.error(f"  âŒ {model.name}: Empty response received")
                    raise ValueError(f"Empty response from {model.name}")

                logger.debug(f"  ğŸ“ {model.name} response (first 200 chars): {result_text[:200]}")

                # JSON íŒŒì‹±
                try:
                    report_data = json.loads(result_text)
                except json.JSONDecodeError as e:
                    logger.error(f"  âŒ {model.name} JSON parse error. Response: {result_text[:500]}")
                    raise

                # StockAnalysisSummary ê°ì²´ ìƒì„±
                summary = StockAnalysisSummary(
                    stock_code=stock_code,
                    model_id=model.id,
                    overall_summary=report_data.get("overall_summary"),
                    short_term_scenario=report_data.get("short_term_scenario"),
                    medium_term_scenario=report_data.get("medium_term_scenario"),
                    long_term_scenario=report_data.get("long_term_scenario"),
                    risk_factors=json.dumps(report_data.get("risk_factors", [])) if isinstance(report_data.get("risk_factors"), list) else report_data.get("risk_factors"),
                    opportunity_factors=json.dumps(report_data.get("opportunity_factors", [])) if isinstance(report_data.get("opportunity_factors"), list) else report_data.get("opportunity_factors"),
                    recommendation=report_data.get("recommendation"),
                    confidence_level=report_data.get("confidence_level", "medium"),
                    data_sources_used=data_sources,  # í†µí•© data_sources
                    limitations=report_data.get("limitations", []),
                    data_completeness_score=available_count / 8.0,  # 8ê°œ ë°ì´í„° ì†ŒìŠ¤ (predictions ì¶”ê°€)
                    total_predictions=total_predictions,
                    based_on_prediction_count=total_predictions,
                    up_count=up_count,
                    down_count=down_count,
                    hold_count=hold_count,
                    # ê°€ê²© ëª©í‘œì¹˜ (ìˆìœ¼ë©´ í¬í•¨)
                    base_price=report_data.get("price_targets", {}).get("base_price"),
                    short_term_target_price=report_data.get("price_targets", {}).get("short_term_target"),
                    short_term_support_price=report_data.get("price_targets", {}).get("short_term_support"),
                    medium_term_target_price=report_data.get("price_targets", {}).get("medium_term_target"),
                    medium_term_support_price=report_data.get("price_targets", {}).get("medium_term_support"),
                    long_term_target_price=report_data.get("price_targets", {}).get("long_term_target"),
                )

                logger.info(f"  âœ… {model.name} report created (confidence={summary.confidence_level}, predictions={total_predictions})")
                return {"success": True, "model": model, "summary": summary}

            except Exception as model_error:
                logger.error(
                    f"  âŒ {model.name} report generation failed: {model_error}",
                    exc_info=True
                )
                return {"success": False, "model": model, "error": str(model_error)}

        # ëª¨ë“  ëª¨ë¸ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        logger.info(f"  ğŸš€ Starting parallel report generation for {len(active_models)} models")
        tasks = [generate_for_single_model(model) for model in active_models]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬
        created_summaries: List[StockAnalysisSummary] = []
        failed_models = []

        for result in results:
            if isinstance(result, Exception):
                logger.error(f"  âŒ Unexpected exception in task: {result}", exc_info=result)
                failed_models.append("unknown")
                continue

            if result.get("success"):
                summary = result["summary"]

                # ê¸°ì¡´ ë¦¬í¬íŠ¸ ì‚­ì œ ë˜ëŠ” ì—…ë°ì´íŠ¸ (ëª¨ë¸ë³„)
                existing = db.query(StockAnalysisSummary).filter(
                    StockAnalysisSummary.stock_code == stock_code,
                    StockAnalysisSummary.model_id == summary.model_id
                ).first()

                if existing:
                    # ê¸°ì¡´ ë¦¬í¬íŠ¸ ì‚­ì œ
                    db.delete(existing)
                    db.flush()

                db.add(summary)
                db.commit()
                db.refresh(summary)
                created_summaries.append(summary)
                logger.info(f"  ğŸ’¾ {result['model'].name} report saved to DB")
            else:
                failed_models.append(result.get("model").name)
                logger.error(f"  âŒ {result.get('model').name} failed: {result.get('error')}")

        # ìµœì¢… ê²°ê³¼
        if created_summaries:
            logger.info(f"âœ… Unified report generation complete: {len(created_summaries)}/{len(active_models)} models succeeded")
        else:
            logger.error(f"âŒ All models failed for {stock_code}")

        if failed_models:
            logger.warning(f"âš ï¸ Failed models: {', '.join(failed_models)}")

        return created_summaries

    except Exception as e:
        logger.error(f"âŒ Unified report generation failed for {stock_code}: {e}", exc_info=True)
        return []


async def create_placeholder_report(stock_code: str, db: Session, error_msg: str):
    """
    ì˜¤ë¥˜ ë°œìƒ ì‹œ placeholder ë¦¬í¬íŠ¸ ìƒì„±

    ë°ì´í„° ì—†ì´ë„ ì¢…ëª©ì´ "ì¶”ì  ì¤‘" ëª©ë¡ì— ë‚˜íƒ€ë‚˜ë„ë¡ í•¨
    """
    summary = StockAnalysisSummary(
        stock_code=stock_code,
        overall_summary="ë°ì´í„° ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.",
        recommendation="ë³´ë¥˜",
        confidence_level="low",
        data_sources_used={
            "market_data": False,
            "investor_trading": False,
            "financial_ratios": False,
            "product_info": False,
            "news": False
        },
        limitations=[f"ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {error_msg}"]
    )

    db.add(summary)
    db.commit()
    logger.info(f"ğŸ“ Placeholder report created for {stock_code}")


async def build_analysis_context_from_db(stock_code: str, db: Session) -> Dict[str, Any]:
    """
    DB ì¿¼ë¦¬ë§Œìœ¼ë¡œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (KIS API í˜¸ì¶œ 0íšŒ)

    Returns:
        {
            "stock_code": "005930",
            "stock_name": "ì‚¼ì„±ì „ì",
            "current_price": {...},
            "investor_trading": [...],
            "financial_ratios": [...],
            "product_info": {...},
            "technical_indicators": {...},
            "news": [...],
            "data_sources": {
                "market_data": True,
                "investor_trading": True,
                "financial_ratios": True,
                "product_info": True,
                "technical_indicators": False,
                "news": True
            }
        }
    """
    logger.debug(f"Building analysis context from DB for {stock_code}")

    context = {
        "stock_code": stock_code,
        "data_sources": {}
    }

    # Stock ê¸°ë³¸ ì •ë³´
    stock = db.query(Stock).filter(Stock.code == stock_code).first()
    if stock:
        context["stock_name"] = stock.name

    # Tier 1: DB ì¿¼ë¦¬ (API í˜¸ì¶œ ì—†ìŒ)

    # 1. í˜„ì¬ê°€
    current_price = db.query(StockCurrentPrice).filter(
        StockCurrentPrice.stock_code == stock_code
    ).order_by(StockCurrentPrice.created_at.desc()).first()

    if current_price:
        context["current_price"] = {
            "current_price": current_price.stck_prpr,
            "change_rate": current_price.prdy_ctrt,
            "volume": current_price.acml_vol,
            "per": current_price.per,
            "pbr": current_price.pbr,
            "eps": current_price.eps,
            "bps": current_price.bps,
            "market_cap": current_price.hts_avls,
        }
    else:
        context["current_price"] = None
    context["data_sources"]["market_data"] = bool(current_price)

    # 2. íˆ¬ìì ìˆ˜ê¸‰ (ìµœê·¼ 5ì¼)
    investor_trading = db.query(InvestorTrading).filter(
        InvestorTrading.stock_code == stock_code
    ).order_by(InvestorTrading.date.desc()).limit(5).all()

    if investor_trading:
        context["investor_trading"] = [{
            "date": it.date.isoformat() if it.date else None,
            "foreigner_net": it.frgn_ntby_qty,
            "institution_net": it.orgn_ntby_qty,
            "individual_net": it.prsn_ntby_qty,
        } for it in investor_trading]
    else:
        context["investor_trading"] = []
    context["data_sources"]["investor_trading"] = bool(investor_trading)

    # 3. ì¬ë¬´ë¹„ìœ¨ (ìµœê·¼ 3ë…„)
    financial_ratios = db.query(FinancialRatio).filter(
        FinancialRatio.stock_code == stock_code
    ).order_by(FinancialRatio.stac_yymm.desc()).limit(3).all()

    if financial_ratios:
        context["financial_ratios"] = [{
            "stac_yymm": fr.stac_yymm,
            "roe_val": fr.roe_val,
            "eps": fr.eps,
            "bps": fr.bps,
            "lblt_rate": fr.lblt_rate,
        } for fr in financial_ratios]
    else:
        context["financial_ratios"] = []
    context["data_sources"]["financial_ratios"] = bool(financial_ratios)

    # 4. ìƒí’ˆì •ë³´
    product_info = db.query(ProductInfo).filter(
        ProductInfo.stock_code == stock_code
    ).first()

    if product_info:
        context["product_info"] = {
            "prdt_name": product_info.prdt_name,
            "prdt_clsf_name": product_info.prdt_clsf_name,
            "prdt_risk_grad_cd": product_info.prdt_risk_grad_cd,
        }
    else:
        context["product_info"] = None
    context["data_sources"]["product_info"] = bool(product_info)

    # Tier 2: ê³„ì‚° (DB ë°ì´í„° ê¸°ë°˜)
    # ê¸°ìˆ ì  ì§€í‘œëŠ” ì¼ë´‰ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ê³„ì‚°
    technical_indicators = None
    try:
        from backend.utils.technical_indicators import calculate_technical_indicators
        technical_indicators = calculate_technical_indicators(stock_code, db)
    except Exception as e:
        logger.debug(f"Technical indicators unavailable for {stock_code}: {e}")

    context["technical_indicators"] = technical_indicators
    context["data_sources"]["technical_indicators"] = bool(technical_indicators)

    # ì‹œì¥ ì§€ìˆ˜ (KOSPI/KOSDAQ)
    from backend.utils.market_index import get_market_indices
    market_indices = get_market_indices(db)
    context["market_indices"] = market_indices

    # Tier 3: ì„ íƒ (ë‰´ìŠ¤)
    news = db.query(NewsArticle).filter(
        NewsArticle.stock_code == stock_code
    ).order_by(NewsArticle.published_at.desc()).limit(10).all()

    if news:
        context["news"] = [{
            "title": n.title,
            "content": n.content,
            "published_at": n.published_at.isoformat() if n.published_at else None,
            "source": n.source,
            "url": n.url,
        } for n in news]
    else:
        context["news"] = []
    context["data_sources"]["news"] = bool(news)

    logger.debug(f"Context built: {context['data_sources']}")
    return context


async def build_unified_context(stock_code: str, db: Session) -> Dict[str, Any]:
    """
    í†µí•© ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ìƒì„± - DB ë°ì´í„° + Prediction ë°ì´í„°

    ëª¨ë“  ê°€ìš© ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ì—†ëŠ” ë°ì´í„°ëŠ” None/[]ë¡œ ë°˜í™˜

    Returns:
        {
            "stock_code": "005930",
            "stock_name": "ì‚¼ì„±ì „ì",

            # DB ë°ì´í„°
            "current_price": {...} or None,
            "investor_trading": [...] or [],
            "financial_ratios": [...] or [],
            "product_info": {...} or None,
            "technical_indicators": {...} or None,
            "market_indices": {...} or None,
            "news": [...] or [],

            # Prediction ë°ì´í„°
            "predictions": {
                "raw_data": [...],
                "statistics": {
                    "total": 15,
                    "positive": 8,
                    "negative": 5,
                    "neutral": 2,
                    "high_impact": 3,
                    "medium_impact": 8,
                    "low_impact": 4,
                    "avg_sentiment": 0.35,
                    "avg_relevance": 0.78
                }
            } or None,

            # ë°ì´í„° ê°€ìš©ì„± ì¶”ì 
            "data_sources": {
                "market_data": True,
                "investor_trading": True,
                "financial_ratios": True,
                "product_info": True,
                "technical_indicators": False,
                "market_indices": True,
                "news": True,
                "predictions": True
            }
        }
    """
    logger.info(f"ğŸ”„ Building unified context for {stock_code}")

    # 1. DB ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©)
    context = await build_analysis_context_from_db(stock_code, db)

    # 2. Prediction ë°ì´í„° ì¶”ê°€ ìˆ˜ì§‘ (ìµœê·¼ 7ì¼)
    seven_days_ago = datetime.now() - timedelta(days=7)
    predictions = (
        db.query(Prediction)
        .filter(
            Prediction.stock_code == stock_code,
            Prediction.created_at >= seven_days_ago
        )
        .order_by(Prediction.created_at.desc())
        .all()
    )

    if predictions:
        # í†µê³„ ê³„ì‚°
        total = len(predictions)

        # ê°ì„± ë°©í–¥ ë¶„í¬ (v2.0 í•„ë“œ ìš°ì„ )
        positive_count = sum(1 for p in predictions if p.sentiment_direction == "positive")
        negative_count = sum(1 for p in predictions if p.sentiment_direction == "negative")
        neutral_count = sum(1 for p in predictions if p.sentiment_direction == "neutral")

        # ì˜í–¥ë„ ë ˆë²¨ ë¶„í¬
        high_impact = sum(1 for p in predictions if p.impact_level in ["high", "critical"])
        medium_impact = sum(1 for p in predictions if p.impact_level == "medium")
        low_impact = sum(1 for p in predictions if p.impact_level == "low")

        # í‰ê·  ê°ì„± ì ìˆ˜ ë° ê´€ë ¨ì„± ì ìˆ˜
        sentiment_scores = [p.sentiment_score for p in predictions if p.sentiment_score is not None]
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0.0

        relevance_scores = [p.relevance_score for p in predictions if p.relevance_score is not None]
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0.0

        # Prediction ë°ì´í„°ë¥¼ contextì— ì¶”ê°€
        context["predictions"] = {
            "raw_data": [
                {
                    "sentiment_direction": p.sentiment_direction,
                    "sentiment_score": p.sentiment_score,
                    "impact_level": p.impact_level,
                    "relevance_score": p.relevance_score,
                    "reasoning": p.reasoning,
                    "created_at": p.created_at.isoformat() if p.created_at else None,
                }
                for p in predictions[:20]  # ìµœê·¼ 20ê±´ë§Œ ìƒì„¸ í¬í•¨
            ],
            "statistics": {
                "total": total,
                "positive": positive_count,
                "negative": negative_count,
                "neutral": neutral_count,
                "high_impact": high_impact,
                "medium_impact": medium_impact,
                "low_impact": low_impact,
                "avg_sentiment": round(avg_sentiment, 2),
                "avg_relevance": round(avg_relevance, 2),
            }
        }
        context["data_sources"]["predictions"] = True
        logger.info(f"âœ… Predictions added: {total}ê±´ (positive: {positive_count}, negative: {negative_count}, neutral: {neutral_count})")
    else:
        context["predictions"] = None
        context["data_sources"]["predictions"] = False
        logger.info(f"â„¹ï¸ No predictions available for {stock_code}")

    # 3. ë°ì´í„° ê°€ìš©ì„± ìš”ì•½
    available_sources = [k for k, v in context["data_sources"].items() if v]
    logger.info(f"ğŸ“Š Available data sources: {', '.join(available_sources)}")

    return context


async def should_update_report(
    stock_code: str,
    db: Session,
    existing_summary: Optional[StockAnalysisSummary],
    predictions: list,
    current_price: Optional[Dict[str, Any]],
    force_update: bool
) -> tuple[bool, str]:
    """
    ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨ (ì‹œì¥ ì‹œê°„ ê¸°ë°˜)

    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        db: Database session
        existing_summary: ê¸°ì¡´ ìš”ì•½ (Noneì´ë©´ ì‹ ê·œ ìƒì„±)
        predictions: ìµœê·¼ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸
        current_price: í˜„ì¬ ì£¼ê°€ ì •ë³´
        force_update: ê°•ì œ ì—…ë°ì´íŠ¸ ì—¬ë¶€

    Returns:
        (ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€, ì‚¬ìœ )
    """
    if force_update or not existing_summary:
        return True, "ê°•ì œ ì—…ë°ì´íŠ¸ ë˜ëŠ” ë¦¬í¬íŠ¸ ì—†ìŒ"

    market_phase = get_market_phase()
    staleness_hours = (datetime.now() - existing_summary.last_updated).total_seconds() / 3600

    # íŠ¸ë¦¬ê±° 1: ì˜ˆì¸¡ ê°œìˆ˜ ì¦ê°€
    total_prediction_count = (
        db.query(func.count(Prediction.id))
        .filter(Prediction.stock_code == stock_code)
        .scalar()
    )

    if existing_summary.based_on_prediction_count < total_prediction_count:
        return True, (
            f"ìƒˆ ì˜ˆì¸¡ ì¶”ê°€ (ê¸°ì¡´: {existing_summary.based_on_prediction_count}, "
            f"í˜„ì¬: {total_prediction_count}, ì‹œì¥: {market_phase})"
        )

    # íŠ¸ë¦¬ê±° 2: ì‹œì¥ ì‹œê°„ ê¸°ë°˜ TTL ì´ˆê³¼
    ttl_hours = get_ttl_hours(market_phase)
    if staleness_hours >= ttl_hours:
        return True, (
            f"ì‹œì¥ ë‹¨ê³„ë³„ TTL ì´ˆê³¼ ({market_phase}: {staleness_hours:.1f}h > {ttl_hours}h)"
        )

    # íŠ¸ë¦¬ê±° 3: ì£¼ê°€ ê¸‰ë³€ (ì¥ì¤‘ë§Œ)
    if market_phase in ["market_open", "trading", "market_close"] and current_price:
        price_threshold = get_price_threshold(market_phase)
        price_change_rate = abs(current_price.get("change_rate", 0))

        if price_change_rate >= price_threshold:
            return True, (
                f"ì£¼ê°€ ê¸‰ë³€ ({price_change_rate:.1f}%, ì„ê³„ê°’: {price_threshold}%, "
                f"ì‹œì¥: {market_phase})"
            )

    # íŠ¸ë¦¬ê±° 4: ì˜ˆì¸¡ ë°©í–¥ ë³€í™”
    if predictions:
        current_up_ratio = sum(1 for p in predictions if p.direction == "up") / len(predictions)
        report_up_ratio = existing_summary.up_count / existing_summary.total_predictions if existing_summary.total_predictions > 0 else 0

        direction_threshold = get_direction_threshold(market_phase)
        direction_change = abs(current_up_ratio - report_up_ratio)

        if direction_change >= direction_threshold:
            return True, (
                f"ì˜ˆì¸¡ ë°©í–¥ ê¸‰ë³€ (ìƒìŠ¹ ë¹„ìœ¨: {report_up_ratio:.1%} â†’ {current_up_ratio:.1%}, "
                f"ë³€í™”: {direction_change:.1%}, ì„ê³„ê°’: {direction_threshold:.1%}, ì‹œì¥: {market_phase})"
            )

    return False, f"ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš” (ì‹œì¥: {market_phase}, ê²½ê³¼: {staleness_hours:.1f}h/{ttl_hours}h)"


def get_stock_analysis_summary(
    stock_code: str,
    db: Session
) -> Optional[Dict[str, Any]]:
    """
    ì¢…ëª© íˆ¬ì ë¶„ì„ ìš”ì•½ ì¡°íšŒ

    Args:
        stock_code: ì¢…ëª© ì½”ë“œ
        db: Database session

    Returns:
        ë¶„ì„ ìš”ì•½ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        from backend.config import settings

        model_map = {
            model.id: model for model in db.query(Model).all()
        }

        ab_config: Optional[ABTestConfig] = None
        if settings.AB_TEST_ENABLED:
            ab_config = (
                db.query(ABTestConfig)
                .filter(ABTestConfig.is_active == True)
                .first()
            )

        if ab_config:
            report_a = _fetch_latest_summary(
                db, stock_code, model_id=ab_config.model_a_id
            )
            report_b = _fetch_latest_summary(
                db, stock_code, model_id=ab_config.model_b_id
            )

            if report_a and report_b:
                formatted_a = _format_summary_output(report_a, model_map)
                formatted_b = _format_summary_output(report_b, model_map)
                comparison = _build_comparison(formatted_a, formatted_b)
                meta_last_updated = max(
                    report_a.last_updated or datetime.min,
                    report_b.last_updated or datetime.min,
                )
                meta_prediction_count = max(
                    report_a.based_on_prediction_count or 0,
                    report_b.based_on_prediction_count or 0,
                )

                return {
                    "ab_test_enabled": True,
                    "model_a": formatted_a,
                    "model_b": formatted_b,
                    "comparison": comparison,
                    "meta": {
                        "last_updated": meta_last_updated.isoformat()
                        if meta_last_updated
                        else None,
                        "based_on_prediction_count": meta_prediction_count,
                    },
                }

        # A/B ë¹„í™œì„±í™” ë˜ëŠ” ëª¨ë¸ ë¦¬í¬íŠ¸ ë¯¸ì¡´ì¬ ì‹œ ìµœì‹  ë¦¬í¬íŠ¸ 1ê±´ ë°˜í™˜
        summary = _fetch_latest_summary(db, stock_code)

        if not summary:
            return None

        return _format_summary_output(summary, model_map)

    except Exception as e:
        logger.error(f"ì¢…ëª© {stock_code}ì˜ ë¶„ì„ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        return None


def _generate_report_for_model(
    generator,
    model: Model,
    prompt: str,
) -> Optional[Dict[str, Any]]:
    """ì£¼ì–´ì§„ ëª¨ë¸ë¡œ LLM ë¦¬í¬íŠ¸ë¥¼ ìƒì„±."""
    try:
        client = generator._create_client(model.provider)
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ í•œêµ­ ì£¼ì‹ ì‹œì¥ì˜ ë² í…Œë‘ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ íˆ¬ì ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.",
            },
            {"role": "user", "content": prompt},
        ]

        kwargs = {
            "model": model.model_identifier,
            "messages": messages,
            "temperature": 0.4,
            "max_tokens": 4000 if model.model_type == "reasoning" else 1000,
        }

        # ì¼ë°˜ ëª¨ë¸ë§Œ response_format ì‚¬ìš© (reasoning ëª¨ë¸ ì œì™¸)
        if model.provider != "openrouter" and model.model_type != "reasoning":
            kwargs["response_format"] = {"type": "json_object"}

        response = client.chat.completions.create(**kwargs)
        result_text = response.choices[0].message.content

        if model.provider == "openrouter":
            result_text = _extract_openrouter_json(result_text)

        return json.loads(result_text)
    except Exception as e:
        logger.error(
            f"ëª¨ë¸ {model.name} ({model.provider}) ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}",
            exc_info=True,
        )
        return None


def _build_summary_from_payload(
    stock_code: str,
    model: Model,
    report_payload: Dict[str, Any],
    total_predictions: int,
    up_count: int,
    down_count: int,
    hold_count: int,
    avg_confidence: Optional[float],
) -> StockAnalysisSummary:
    """LLM ì‘ë‹µì„ StockAnalysisSummary ì—”í‹°í‹°ë¡œ ë³€í™˜."""
    price_targets = report_payload.get("price_targets") or {}
    now = datetime.now()

    # JSON í•„ë“œëŠ” ì§ë ¬í™”ëœ ë¬¸ìì—´ë¡œ ì €ì¥
    risk_factors = report_payload.get("risk_factors", [])
    opportunity_factors = report_payload.get("opportunity_factors", [])

    return StockAnalysisSummary(
        stock_code=stock_code,
        model_id=model.id,
        overall_summary=report_payload.get("overall_summary"),
        short_term_scenario=report_payload.get("short_term_scenario"),
        medium_term_scenario=report_payload.get("medium_term_scenario"),
        long_term_scenario=report_payload.get("long_term_scenario"),
        risk_factors=json.dumps(risk_factors) if isinstance(risk_factors, list) else risk_factors,
        opportunity_factors=json.dumps(opportunity_factors) if isinstance(opportunity_factors, list) else opportunity_factors,
        recommendation=report_payload.get("recommendation"),
        custom_data=json.dumps({
            "model_id": model.id,
            "model_name": model.name,
            "raw_report": report_payload,
        }),
        total_predictions=total_predictions,
        up_count=up_count,
        down_count=down_count,
        hold_count=hold_count,
        avg_confidence=avg_confidence,
        base_price=price_targets.get("base_price"),
        short_term_target_price=price_targets.get("short_term_target"),
        short_term_support_price=price_targets.get("short_term_support"),
        medium_term_target_price=price_targets.get("medium_term_target"),
        medium_term_support_price=price_targets.get("medium_term_support"),
        long_term_target_price=price_targets.get("long_term_target"),
        last_updated=now,
        based_on_prediction_count=total_predictions,
    )


def _extract_openrouter_json(text: str) -> str:
    """
    OpenRouter ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ.
    ì¤‘ì²©ëœ ê°ì²´ì™€ ë¶ˆì™„ì „í•œ ì‘ë‹µë„ ì²˜ë¦¬.
    """
    if not text:
        return text

    # 1. Markdown ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ (```json ... ``` ë˜ëŠ” ``` ... ```)
    # Greedy ë§¤ì¹­ìœ¼ë¡œ ë§ˆì§€ë§‰ ë‹«ëŠ” ë°±í‹±ê¹Œì§€ í¬í•¨
    json_match = re.search(r"```(?:json)?\s*(\{.*\})\s*```", text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()

    # 2. ì½”ë“œ ë¸”ë¡ ì‹œì‘ë§Œ ìˆê³  ëì´ ì—†ëŠ” ê²½ìš° (ì‘ë‹µ truncation)
    json_match = re.search(r"```(?:json)?\s*(\{.*)", text, re.DOTALL)
    if json_match:
        potential_json = json_match.group(1).strip()
        # JSON ìœ íš¨ì„± ê°„ë‹¨ ì²´í¬ - overall_summary í‚¤ê°€ ìˆëŠ”ì§€
        if '"overall_summary"' in potential_json:
            return potential_json

    # 3. ì½”ë“œ ë¸”ë¡ ì—†ì´ ì§ì ‘ JSON (overall_summaryë¡œ ì‹œì‘í•˜ëŠ” ê°ì²´)
    json_match = re.search(r'(\{[^{]*"overall_summary".*\})', text, re.DOTALL)
    if json_match:
        return json_match.group(1).strip()

    # 4. fallback - ì›ë³¸ ë°˜í™˜
    return text.strip()


def _fetch_latest_summary(
    db: Session,
    stock_code: str,
    model_id: Optional[int] = None,
) -> Optional[StockAnalysisSummary]:
    """íŠ¹ì • ì¢…ëª©(+ëª¨ë¸)ì˜ ìµœì‹  ë¦¬í¬íŠ¸ ì¡°íšŒ."""
    query = db.query(StockAnalysisSummary).filter(
        StockAnalysisSummary.stock_code == stock_code
    )
    if model_id is not None:
        query = query.filter(StockAnalysisSummary.model_id == model_id)

    return query.order_by(StockAnalysisSummary.last_updated.desc()).first()


def _format_summary_output(
    summary: StockAnalysisSummary,
    model_map: Dict[int, Model],
) -> Dict[str, Any]:
    """StockAnalysisSummary ì—”í‹°í‹°ë¥¼ API ì‘ë‹µ í˜•íƒœë¡œ ë³€í™˜."""
    model_info = model_map.get(summary.model_id) if summary.model_id else None
    statistics = {
        "total_predictions": summary.total_predictions,
        "up_count": summary.up_count,
        "down_count": summary.down_count,
        "hold_count": summary.hold_count,
        "avg_confidence": round(summary.avg_confidence * 100, 1)
        if summary.avg_confidence
        else None,
    }
    price_targets = {
        "base_price": summary.base_price,
        "short_term_target": summary.short_term_target_price,
        "short_term_support": summary.short_term_support_price,
        "medium_term_target": summary.medium_term_target_price,
        "medium_term_support": summary.medium_term_support_price,
        "long_term_target": summary.long_term_target_price,
    }

    # JSON ë¬¸ìì—´ íŒŒì‹±
    risk_factors = summary.risk_factors
    if isinstance(risk_factors, str):
        try:
            risk_factors = json.loads(risk_factors)
        except:
            risk_factors = []
    elif risk_factors is None:
        risk_factors = []

    opportunity_factors = summary.opportunity_factors
    if isinstance(opportunity_factors, str):
        try:
            opportunity_factors = json.loads(opportunity_factors)
        except:
            opportunity_factors = []
    elif opportunity_factors is None:
        opportunity_factors = []

    # US-004: ë©”íƒ€ë°ì´í„° íŒŒì‹±
    data_sources_used = summary.data_sources_used
    if isinstance(data_sources_used, str):
        try:
            data_sources_used = json.loads(data_sources_used)
        except:
            data_sources_used = None

    # ë°±ì—”ë“œ â†’ í”„ë¡ íŠ¸ì—”ë“œ í‚¤ ë§¤í•‘ ë° ë°°ì—´ ë³€í™˜
    backend_to_frontend_keys = {
        "market_data": "stock_prices",
        "investor_trading": "investor_flow",
        "financial_ratios": "financial_metrics",
        "product_info": "company_info",
        "technical_indicators": "technical_indicators",
        "news": "market_trends",
        "predictions": None,  # í”„ë¡ íŠ¸ì—”ë“œì— í‘œì‹œ ì•ˆí•¨
    }

    # dict -> array ë³€í™˜ (Trueì¸ ê°’ë§Œ ì¶”ì¶œí•˜ê³  í”„ë¡ íŠ¸ì—”ë“œ í‚¤ë¡œ ë§¤í•‘)
    data_sources_array = []
    if isinstance(data_sources_used, dict):
        for backend_key, is_used in data_sources_used.items():
            if is_used and backend_key in backend_to_frontend_keys:
                frontend_key = backend_to_frontend_keys[backend_key]
                if frontend_key:  # Noneì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                    data_sources_array.append(frontend_key)
        data_sources_used = data_sources_array

    limitations = summary.limitations
    if isinstance(limitations, str):
        try:
            limitations = json.loads(limitations)
        except:
            limitations = []
    elif limitations is None:
        limitations = []

    return {
        "model_id": summary.model_id,
        "model_name": model_info.name if model_info else None,
        "overall_summary": summary.overall_summary,
        "short_term_scenario": summary.short_term_scenario,
        "medium_term_scenario": summary.medium_term_scenario,
        "long_term_scenario": summary.long_term_scenario,
        "risk_factors": risk_factors,
        "opportunity_factors": opportunity_factors,
        "recommendation": summary.recommendation,
        "price_targets": price_targets,
        "statistics": statistics,
        # US-004: ë©”íƒ€ë°ì´í„° ì¶”ê°€
        "confidence_level": summary.confidence_level,
        "data_sources_used": data_sources_used,  # ì´ì œ ë°°ì—´ í˜•íƒœ
        "limitations": limitations,
        "data_completeness_score": summary.data_completeness_score,
        "meta": {
            "last_updated": summary.last_updated.isoformat()
            if summary.last_updated
            else None,
            "based_on_prediction_count": summary.based_on_prediction_count,
        },
    }


def _build_comparison(
    report_a: Dict[str, Any],
    report_b: Dict[str, Any],
) -> Dict[str, Any]:
    """ë‘ ë¦¬í¬íŠ¸ë¥¼ ë¹„êµí•˜ì—¬ ê³µí†µì  ë„ì¶œ."""
    rec_a = (report_a.get("recommendation") or "").lower()
    rec_b = (report_b.get("recommendation") or "").lower()

    recommendation_match = False
    if ("ë§¤ìˆ˜" in rec_a and "ë§¤ìˆ˜" in rec_b) or \
       ("ë§¤ë„" in rec_a and "ë§¤ë„" in rec_b) or \
       ("ê´€ë§" in rec_a and "ê´€ë§" in rec_b) or \
       ("ë³´ìœ " in rec_a and "ë³´ìœ " in rec_b):
        recommendation_match = True

    risks_a = set(report_a.get("risk_factors") or [])
    risks_b = set(report_b.get("risk_factors") or [])
    opps_a = set(report_a.get("opportunity_factors") or [])
    opps_b = set(report_b.get("opportunity_factors") or [])

    return {
        "recommendation_match": recommendation_match,
        "risk_overlap": list(risks_a & risks_b),
        "opportunity_overlap": list(opps_a & opps_b),
    }
