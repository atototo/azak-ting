"""
ë‰´ìŠ¤ ë²¡í„° ê²€ìƒ‰ ëª¨ë“ˆ (AsyncIO ì™„ì „ ì¬ì„¤ê³„)

FAISSì—ì„œ ìœ ì‚¬í•œ ê³¼ê±° ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³ , í•´ë‹¹ ë‰´ìŠ¤ì˜ ì£¼ê°€ ë³€ë™ë¥ ì„ ì¡°íšŒí•©ë‹ˆë‹¤.

ì„¤ê³„ ì›ì¹™:
- Singleton íŒ¨í„´ìœ¼ë¡œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ ë³´ì¥
- ì™„ì „ ë¹„ë™ê¸° (async/await)
- ModelLoadLockìœ¼ë¡œ PyTorch ë™ì‹œì„± ì œì–´
- ì´ˆê¸°í™” ì‹œ FAISS ì¸ë±ìŠ¤ í•œ ë²ˆë§Œ ë¡œë“œ
- CPU ì§‘ì•½ ì‘ì—…ì€ executorë¡œ ë¶„ë¦¬

ì¸ë±ìŠ¤ ìµœì í™” (Issue #19):
- IndexIVFFlat + Inner Product ì‚¬ìš© (10,000ê±´ ì´ìƒ ìµœì )
- Inner Product = Cosine Similarity (L2 ì •ê·œí™”ëœ ë²¡í„°)
- í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ O(N) â†’ O(âˆšN) ì„±ëŠ¥ ê°œì„ 
"""
import logging
import os
import pickle
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

import faiss
import numpy as np
from sqlalchemy.orm import Session

from backend.config import settings
from backend.llm.embedder import get_news_embedder
from backend.db.models.news import NewsArticle
from backend.db.models.match import NewsStockMatch
from backend.llm.model_lock import ModelLoadLock


logger = logging.getLogger(__name__)


class NewsVectorSearch:
    """
    ë‰´ìŠ¤ ë²¡í„° ê²€ìƒ‰ í´ë˜ìŠ¤ - FAISS ê¸°ë°˜ (Singleton + ì™„ì „ ë¹„ë™ê¸°)

    Features:
    - Singleton íŒ¨í„´ìœ¼ë¡œ ì „ì—­ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤
    - ModelLoadLockìœ¼ë¡œ PyTorch Segmentation Fault ë°©ì§€
    - ì´ˆê¸°í™” ì‹œ FAISS ì¸ë±ìŠ¤ í•œ ë²ˆë§Œ ë¡œë“œ
    - ëª¨ë“  ë©”ì„œë“œ async/await
    - IndexIVFFlat + Inner Product (Issue #19)
    """

    _instance: Optional['NewsVectorSearch'] = None
    _initialized: bool = False

    # IVF ì¸ë±ìŠ¤ ì„¤ì •
    IVF_NLIST = 100  # í´ëŸ¬ìŠ¤í„° ìˆ˜ (sqrt(N) ê¶Œì¥, 10000ê±´ â†’ 100)
    IVF_NPROBE = 10  # ê²€ìƒ‰ ì‹œ íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
    MIN_VECTORS_FOR_IVF = 1000  # IVF í•™ìŠµì— í•„ìš”í•œ ìµœì†Œ ë²¡í„° ìˆ˜

    def __new__(cls):
        """Singleton íŒ¨í„´ êµ¬í˜„"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        """ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ ì‹¤í–‰)"""
        if NewsVectorSearch._initialized:
            return

        self.embedder = get_news_embedder()
        self.index_path = settings.FAISS_INDEX_PATH
        self.metadata_path = settings.FAISS_METADATA_PATH
        self._index: Optional[faiss.Index] = None
        self._metadata: List[Dict[str, Any]] = []
        self._is_ivf: bool = False  # IVF ì¸ë±ìŠ¤ ì—¬ë¶€

        NewsVectorSearch._initialized = True
        logger.info("ğŸ” NewsVectorSearch ì´ˆê¸°í™” ì™„ë£Œ (Singleton)")

    def _ensure_index_dir(self):
        """ì¸ë±ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
        os.makedirs(os.path.dirname(self.metadata_path), exist_ok=True)

    def _create_empty_index(self) -> faiss.Index:
        """ë¹ˆ IndexFlatIP ì¸ë±ìŠ¤ ìƒì„± (Inner Product)"""
        return faiss.IndexFlatIP(settings.EMBEDDING_DIM)

    def _is_index_ivf(self, index: faiss.Index) -> bool:
        """ì¸ë±ìŠ¤ê°€ IVF íƒ€ì…ì¸ì§€ í™•ì¸"""
        try:
            # IVF ì¸ë±ìŠ¤ëŠ” nprobe ì†ì„±ì„ ê°€ì§
            _ = index.nprobe
            return True
        except AttributeError:
            return False

    async def load_index(self):
        """
        FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ (ModelLoadLock ì ìš©)

        PyTorch Segmentation Fault ë°©ì§€ë¥¼ ìœ„í•´ Lock ì‚¬ìš©
        ê¸°ì¡´ IndexFlatL2ì™€ ìƒˆë¡œìš´ IndexIVFFlat ëª¨ë‘ ì§€ì›
        """
        async with ModelLoadLock.get_lock():
            if self._index is not None:
                logger.debug("FAISS ì¸ë±ìŠ¤ ì´ë¯¸ ë¡œë“œë¨ (ì¬ì‚¬ìš©)")
                return

            if not os.path.exists(self.index_path) or not os.path.exists(self.metadata_path):
                logger.warning("FAISS ì¸ë±ìŠ¤ íŒŒì¼ ì—†ìŒ, ë¹ˆ ì¸ë±ìŠ¤ ì´ˆê¸°í™”")
                self._index = self._create_empty_index()
                self._metadata = []
                self._is_ivf = False
                return

            try:
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self._index = faiss.read_index(self.index_path)
                self._is_ivf = self._is_index_ivf(self._index)

                # IVF ì¸ë±ìŠ¤ë©´ nprobe ì„¤ì •
                if self._is_ivf:
                    self._index.nprobe = self.IVF_NPROBE
                    logger.info(f"ğŸ“Š IVF ì¸ë±ìŠ¤ ë¡œë“œë¨ (nprobe={self.IVF_NPROBE})")

                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(self.metadata_path, 'rb') as f:
                    self._metadata = pickle.load(f)

                index_type = "IVFFlat+IP" if self._is_ivf else "FlatL2(legacy)"
                logger.info(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {self._index.ntotal}ê°œ ë²¡í„° ({index_type})")

            except Exception as e:
                logger.error(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
                self._index = self._create_empty_index()
                self._metadata = []
                self._is_ivf = False

    def save_index(self):
        """
        FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì €ì¥

        Note: ë™ê¸° í•¨ìˆ˜ (íŒŒì¼ I/OëŠ” CPU-bound ì‘ì—…ì´ë¯€ë¡œ ë™ê¸° ì²˜ë¦¬)
        """
        try:
            self._ensure_index_dir()

            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self._index, self.index_path)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(self._metadata, f)

            logger.info(f"ğŸ’¾ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {self._index.ntotal}ê°œ ë²¡í„°")

        except Exception as e:
            logger.error(f"âŒ FAISS ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def add_embeddings(
        self,
        news_ids: List[int],
        embeddings: List[List[float]],
        stock_codes: List[str],
        published_timestamps: List[int],
    ) -> int:
        """
        ì„ë² ë”©ì„ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€ (ë¹„ë™ê¸°)

        Args:
            news_ids: ë‰´ìŠ¤ ID ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
            stock_codes: ì¢…ëª© ì½”ë“œ ë¦¬ìŠ¤íŠ¸
            published_timestamps: ë°œí–‰ ì‹œê° (Unix timestamp) ë¦¬ìŠ¤íŠ¸

        Returns:
            ì¶”ê°€ëœ ë²¡í„° ìˆ˜
        """
        if len(news_ids) != len(embeddings) != len(stock_codes) != len(published_timestamps):
            logger.error("ì…ë ¥ ë¦¬ìŠ¤íŠ¸ ê¸¸ì´ ë¶ˆì¼ì¹˜")
            return 0

        try:
            await self.load_index()  # â† Lockìœ¼ë¡œ ë³´í˜¸

            # numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_np = np.array(embeddings, dtype=np.float32)

            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            self._index.add(embeddings_np)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for news_id, stock_code, timestamp in zip(news_ids, stock_codes, published_timestamps):
                self._metadata.append({
                    "news_article_id": news_id,
                    "stock_code": stock_code,
                    "published_at": timestamp,
                })

            # ì €ì¥
            self.save_index()

            logger.info(f"âœ… FAISS ì¸ë±ìŠ¤ì— {len(news_ids)}ê°œ ë²¡í„° ì¶”ê°€")
            return len(news_ids)

        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return 0

    async def get_indexed_news_ids(self) -> set:
        """
        FAISSì— ì´ë¯¸ ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ëª©ë¡ ë°˜í™˜ (ë¹„ë™ê¸°)

        Returns:
            ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ì§‘í•©
        """
        try:
            await self.load_index()  # â† Lockìœ¼ë¡œ ë³´í˜¸

            if not self._metadata:
                return set()

            return set(meta["news_article_id"] for meta in self._metadata)

        except Exception as e:
            logger.error(f"âŒ ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return set()

    async def search_similar_news(
        self,
        news_text: str,
        stock_code: Optional[str] = None,
        top_k: int = 5,
        similarity_threshold: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬í•œ ê³¼ê±° ë‰´ìŠ¤ ê²€ìƒ‰ (ë¹„ë™ê¸°)

        Args:
            news_text: ê²€ìƒ‰í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            stock_code: ì¢…ëª© ì½”ë“œ í•„í„° (Noneì´ë©´ ì „ì²´ ê²€ìƒ‰)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)

        Returns:
            ìœ ì‚¬ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [
                {
                    "news_id": int,
                    "similarity": float,
                    "stock_code": str,
                    "published_at": int
                },
                ...
            ]
        """
        try:
            await self.load_index()  # â† Lockìœ¼ë¡œ ë³´í˜¸

            if self._index.ntotal == 0:
                logger.warning("FAISS ì¸ë±ìŠ¤ ë¹„ì–´ìˆìŒ")
                return []

            # ì„ë² ë”© ìƒì„± (CPU ì§‘ì•½ì  ì‘ì—…ì´ë¯€ë¡œ executorì—ì„œ ì‹¤í–‰)
            loop = asyncio.get_event_loop()
            query_embedding = await loop.run_in_executor(
                None, self.embedder.embed_text, news_text
            )
            query_vector = np.array([query_embedding], dtype=np.float32)

            # FAISS ê²€ìƒ‰
            search_k = top_k * 10 if stock_code else top_k
            logger.debug(f"FAISS ê²€ìƒ‰ ì‹œì‘: query_vector shape={query_vector.shape}, k={search_k}")
            distances, indices = self._index.search(query_vector, search_k)
            logger.debug(f"FAISS ê²€ìƒ‰ ì™„ë£Œ: found {len(indices[0])} items")

            # ê²°ê³¼ í•„í„°ë§ ë° ë³€í™˜
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                if idx < 0 or idx >= len(self._metadata):
                    continue

                meta = self._metadata[idx]

                # ì¢…ëª© ì½”ë“œ í•„í„°
                if stock_code and meta["stock_code"] != stock_code:
                    continue

                # ìœ ì‚¬ë„ ê³„ì‚°
                # - Inner Product (IVF): ì •ê·œí™”ëœ ë²¡í„°ëŠ” IP = cosine similarity
                # - L2 distance (legacy): ê·¼ì‚¬ ë³€í™˜
                if self._is_ivf:
                    similarity = float(dist)  # IPëŠ” ë°”ë¡œ similarity
                else:
                    similarity = 1 / (1 + dist)  # L2 â†’ similarity ê·¼ì‚¬

                if similarity < similarity_threshold:
                    continue

                results.append({
                    "news_id": meta["news_article_id"],
                    "similarity": round(similarity, 4),
                    "stock_code": meta["stock_code"],
                    "published_at": meta.get("published_at"),  # ê¸°ì¡´ ì¸ë±ìŠ¤ í˜¸í™˜ì„± (None í—ˆìš©)
                })

                if len(results) >= top_k:
                    break

            logger.debug(f"ğŸ” ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê±´")
            return results

        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def get_news_with_price_changes(
        self,
        news_text: str,
        stock_code: Optional[str] = None,
        db: Optional[Session] = None,
        top_k: int = 5,
        similarity_threshold: float = 0.7,
    ) -> List[Dict[str, Any]]:
        """
        ìœ ì‚¬ ë‰´ìŠ¤ì™€ í•´ë‹¹ ë‰´ìŠ¤ì˜ ì£¼ê°€ ë³€ë™ë¥  í•¨ê»˜ ì¡°íšŒ (ë¹„ë™ê¸°)

        Args:
            news_text: ê²€ìƒ‰í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸
            stock_code: ì¢…ëª© ì½”ë“œ í•„í„°
            db: DB ì„¸ì…˜
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’

        Returns:
            ìœ ì‚¬ ë‰´ìŠ¤ ë° ì£¼ê°€ ë³€ë™ë¥  ë¦¬ìŠ¤íŠ¸ [
                {
                    "news_id": int,
                    "similarity": float,
                    "news_title": str,
                    "news_content": str,
                    "stock_code": str,
                    "published_at": datetime,
                    "price_changes": {
                        "1d": float or None,
                        "2d": float or None,
                        "3d": float or None,
                        "5d": float or None,
                        "10d": float or None,
                        "20d": float or None
                    }
                },
                ...
            ]
        """
        try:
            # ìœ ì‚¬ ë‰´ìŠ¤ ê²€ìƒ‰
            similar_news = await self.search_similar_news(
                news_text=news_text,
                stock_code=stock_code,
                top_k=top_k,
                similarity_threshold=similarity_threshold,
            )

            if not similar_news or not db:
                return similar_news

            # DBì—ì„œ ìƒì„¸ ì •ë³´ ë° ì£¼ê°€ ë³€ë™ë¥  ì¡°íšŒ
            result = []
            for news in similar_news:
                news_id = news["news_id"]

                # ë‰´ìŠ¤ ìƒì„¸ ì •ë³´
                news_article = db.query(NewsArticle).filter(NewsArticle.id == news_id).first()
                if not news_article:
                    continue

                # ì£¼ê°€ ë³€ë™ë¥  ì¡°íšŒ
                match = (
                    db.query(NewsStockMatch)
                    .filter(NewsStockMatch.news_id == news_id)
                    .first()
                )

                price_changes = {}
                if match:
                    price_changes = {
                        "1d": match.price_change_1d,
                        "2d": match.price_change_2d,
                        "3d": match.price_change_3d,
                        "5d": match.price_change_5d,
                        "10d": match.price_change_10d,
                        "20d": match.price_change_20d,
                    }
                else:
                    price_changes = {
                        "1d": None,
                        "2d": None,
                        "3d": None,
                        "5d": None,
                        "10d": None,
                        "20d": None,
                    }

                result.append({
                    "news_id": news_id,
                    "similarity": news["similarity"],
                    "news_title": news_article.title,
                    "news_content": news_article.content,
                    "stock_code": news_article.stock_code,
                    "published_at": news_article.published_at,
                    "price_changes": price_changes,
                })

            logger.debug(f"ğŸ“Š ì£¼ê°€ ë³€ë™ë¥  í¬í•¨ ê²€ìƒ‰ ì™„ë£Œ: {len(result)}ê±´")
            return result

        except Exception as e:
            logger.error(f"âŒ ì£¼ê°€ ë³€ë™ë¥  ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []


# Singleton ì¸ìŠ¤í„´ìŠ¤
_vector_search_instance: Optional[NewsVectorSearch] = None


async def get_vector_search() -> NewsVectorSearch:
    """
    NewsVectorSearch Singleton ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ë¹„ë™ê¸°)

    ìµœì´ˆ í˜¸ì¶œ ì‹œ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

    Returns:
        NewsVectorSearch ì¸ìŠ¤í„´ìŠ¤
    """
    global _vector_search_instance

    if _vector_search_instance is None:
        _vector_search_instance = NewsVectorSearch()
        await _vector_search_instance.load_index()  # ì´ˆê¸° ë¡œë“œ

    return _vector_search_instance
