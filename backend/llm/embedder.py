"""
ë‰´ìŠ¤ ì„ë² ë”© ëª¨ë“ˆ

HuggingFace Transformers (BM-K/KoSimCSE-roberta)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.
ë¡œì»¬ í•œê¸€ ì„ë² ë”© ëª¨ë¸ë¡œ OpenAI API ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ ê°œì„ 
"""
import logging
from typing import List, Optional, Dict, Tuple
from datetime import datetime
import time
import os

import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from sqlalchemy.orm import Session

from backend.config import settings
from backend.db.models.news import NewsArticle
from backend.db.session import SessionLocal


logger = logging.getLogger(__name__)


class NewsEmbedder:
    """ë‰´ìŠ¤ ì„ë² ë”© í´ë˜ìŠ¤ - ë¡œì»¬ í•œê¸€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©"""

    def __init__(self):
        """ì„ë² ë” ì´ˆê¸°í™” - ëª¨ë¸ì€ ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ"""
        self.model_name = settings.EMBEDDING_MODEL_NAME
        self.embedding_dim = 768  # KoSimCSE-robertaì˜ ì°¨ì›
        self._tokenizer = None
        self._model = None

    @property
    def tokenizer(self):
        """í† í¬ë‚˜ì´ì € lazy loading"""
        if self._tokenizer is None:
            logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {self.model_name}")
            self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            logger.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        return self._tokenizer

    @property
    def model(self):
        """ëª¨ë¸ lazy loading"""
        if self._model is None:
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
            start = time.time()
            self._model = AutoModel.from_pretrained(self.model_name)
            self._model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            load_time = time.time() - start
            logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
        return self._model

    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - ëª¨ë“  í† í°ì˜ ì„ë² ë”© í‰ê· """
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

    def embed_text(self, text: str) -> Optional[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            768ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            # í† í¬ë‚˜ì´ì§•
            encoded_input = self.tokenizer(
                text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )

            # ì„ë² ë”© ìƒì„± (gradient ê³„ì‚° ë¹„í™œì„±í™”)
            with torch.no_grad():
                model_output = self.model(**encoded_input)

            # Mean pooling
            embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])

            # ì •ê·œí™” (L2 normalization)
            embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)

            # numpy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
            embedding_list = embedding.cpu().numpy()[0].tolist()

            logger.debug(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding_list)}ì°¨ì›")

            return embedding_list

        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            return None

    def embed_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤.

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ í•­ëª©ì€ None)
        """
        embeddings = []

        for text in texts:
            embedding = self.embed_text(text)
            embeddings.append(embedding)

            # ë¡œì»¬ ëª¨ë¸ì´ë¯€ë¡œ rate limit ì—†ìŒ (sleep ì œê±°)

        return embeddings

    def get_unembedded_news(self, db: Session, limit: int = 100) -> List[NewsArticle]:
        """
        ì•„ì§ ì„ë² ë”©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜

        Returns:
            ì„ë² ë”©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # FAISSì—ì„œ ì´ë¯¸ ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ì¡°íšŒ
            from backend.llm.vector_search import get_vector_search

            vector_search = get_vector_search()
            embedded_news_ids = vector_search.get_indexed_news_ids()

            logger.info(f"FAISSì— ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤: {len(embedded_news_ids)}ê±´")

        except Exception as e:
            logger.warning(f"FAISS ì¡°íšŒ ì‹¤íŒ¨ (ëª¨ë“  ë‰´ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬): {e}")
            embedded_news_ids = set()

        # PostgreSQLì—ì„œ ë¯¸ì„ë² ë”© ë‰´ìŠ¤ ì¡°íšŒ
        if embedded_news_ids:
            unembedded_news = (
                db.query(NewsArticle)
                .filter(NewsArticle.id.notin_(embedded_news_ids))
                .order_by(NewsArticle.published_at.desc())
                .limit(limit)
                .all()
            )
        else:
            unembedded_news = (
                db.query(NewsArticle)
                .order_by(NewsArticle.published_at.desc())
                .limit(limit)
                .all()
            )

        logger.info(f"ë¯¸ì„ë² ë”© ë‰´ìŠ¤: {len(unembedded_news)}ê±´")
        return unembedded_news

    def save_to_faiss(
        self, news_list: List[NewsArticle], embeddings: List[List[float]]
    ) -> int:
        """
        ë‰´ìŠ¤ ì„ë² ë”©ì„ FAISSì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if len(news_list) != len(embeddings):
            logger.error("ë‰´ìŠ¤ì™€ ì„ë² ë”© ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return 0

        try:
            # FAISSì— ì €ì¥
            from backend.llm.vector_search import get_vector_search

            vector_search = get_vector_search()

            # ë°ì´í„° ì¤€ë¹„
            news_ids = [news.id for news in news_list]
            stock_codes = [news.stock_code or "" for news in news_list]
            published_timestamps = [
                int(news.published_at.timestamp()) for news in news_list
            ]

            # FAISSì— ì¶”ê°€
            saved_count = vector_search.add_embeddings(
                news_ids=news_ids,
                embeddings=embeddings,
                stock_codes=stock_codes,
                published_timestamps=published_timestamps,
            )

            logger.info(f"FAISSì— {saved_count}ê±´ ì €ì¥ ì™„ë£Œ")
            return saved_count

        except Exception as e:
            logger.error(f"FAISS ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0

    def embed_and_save_news(
        self, db: Session, batch_size: int = 100
    ) -> Tuple[int, int]:
        """
        ë¯¸ì„ë² ë”© ë‰´ìŠ¤ë¥¼ ì„ë² ë”©í•˜ì—¬ Milvusì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜) íŠœí”Œ
        """
        logger.info("=" * 60)
        logger.info("ğŸ”¤ ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—… ì‹œì‘")
        logger.info("=" * 60)

        try:
            # ë¯¸ì„ë² ë”© ë‰´ìŠ¤ ì¡°íšŒ
            unembedded_news = self.get_unembedded_news(db, limit=batch_size)

            if not unembedded_news:
                logger.info("ì„ë² ë”©í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return 0, 0

            logger.info(f"ì„ë² ë”© ëŒ€ìƒ ë‰´ìŠ¤: {len(unembedded_news)}ê±´")

            # í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë³¸ë¬¸)
            texts = [f"{news.title}\n{news.content}" for news in unembedded_news]

            # ì„ë² ë”© ìƒì„±
            logger.info("ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            embeddings = self.embed_batch(texts)

            # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¥˜
            success_news = []
            success_embeddings = []
            fail_count = 0

            for news, embedding in zip(unembedded_news, embeddings):
                if embedding is not None:
                    success_news.append(news)
                    success_embeddings.append(embedding)
                else:
                    fail_count += 1
                    logger.warning(f"ë‰´ìŠ¤ ID {news.id} ì„ë² ë”© ì‹¤íŒ¨")

            # FAISSì— ì €ì¥
            if success_embeddings:
                saved_count = self.save_to_faiss(success_news, success_embeddings)
                logger.info(
                    f"âœ… ì„ë² ë”© ì™„ë£Œ: ì„±ê³µ {saved_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´"
                )
                return saved_count, fail_count
            else:
                logger.warning("ì €ì¥í•  ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
                return 0, fail_count

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—… ì¤‘ ì—ëŸ¬: {e}", exc_info=True)
            return 0, 0


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_news_embedder: Optional[NewsEmbedder] = None


def get_news_embedder() -> NewsEmbedder:
    """
    NewsEmbedder ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        NewsEmbedder ì¸ìŠ¤í„´ìŠ¤
    """
    global _news_embedder
    if _news_embedder is None:
        _news_embedder = NewsEmbedder()
    return _news_embedder


def run_daily_embedding(batch_size: int = 100) -> Tuple[int, int]:
    """
    ì¼ì¼ ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)

    Returns:
        (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜) íŠœí”Œ
    """
    db = SessionLocal()
    embedder = get_news_embedder()

    try:
        return embedder.embed_and_save_news(db, batch_size=batch_size)
    finally:
        db.close()
