"""
ë‰´ìŠ¤ ì„ë² ë”© ëª¨ë“ˆ

HuggingFace Transformers (BM-K/KoSimCSE-roberta)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤.
ë¡œì»¬ í•œê¸€ ì„ë² ë”© ëª¨ë¸ë¡œ OpenAI API ë¹„ìš© ì ˆê° ë° ì„±ëŠ¥ ê°œì„ 
"""
import logging
import threading
import pickle
from typing import List, Optional, Dict, Tuple, Any
from datetime import datetime
import time
import os

import torch
import faiss
from transformers import AutoTokenizer, AutoModel
import numpy as np
from sqlalchemy.orm import Session

from backend.config import settings
from backend.db.models.news import NewsArticle
from backend.db.session import SessionLocal


logger = logging.getLogger(__name__)


class NewsEmbedder:
    """ë‰´ìŠ¤ ì„ë² ë”© í´ë˜ìŠ¤ - ë¡œì»¬ í•œê¸€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (Thread-safe)"""

    def __init__(self):
        """ì„ë² ë” ì´ˆê¸°í™” - ëª¨ë¸ì€ ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ"""
        # HuggingFace tokenizer fork ê²½ê³  ë°©ì§€
        os.environ["TOKENIZERS_PARALLELISM"] = "false"

        self.model_name = settings.EMBEDDING_MODEL_NAME
        self.embedding_dim = 768  # KoSimCSE-robertaì˜ ì°¨ì›
        self._tokenizer = None
        self._model = None
        self._inference_lock = threading.Lock()  # PyTorch ë™ì‹œ ì¶”ë¡  ë°©ì§€

    @property
    def tokenizer(self):
        """í† í¬ë‚˜ì´ì € lazy loading (Thread-safe)"""
        if self._tokenizer is None:
            with self._inference_lock:
                if self._tokenizer is None:
                    logger.info(f"í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {self.model_name}")
                    self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                    logger.info("í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
        return self._tokenizer

    @property
    def model(self):
        """ëª¨ë¸ lazy loading (Thread-safe)"""
        if self._model is None:
            with self._inference_lock:
                if self._model is None:
                    logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_name}")
                    start = time.time()
                    self._model = AutoModel.from_pretrained(self.model_name)
                    self._model.eval()  # í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                    load_time = time.time() - start
                    logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({load_time:.2f}ì´ˆ)")
        return self._model

    def _mean_pooling(self, model_output, attention_mask):
        """Mean Pooling - ëª¨ë“  í† í°ì˜ ì„ë² ë”© í‰ê· """
        token_embeddings = model_output[0]
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

    def embed_text(self, text: str) -> Optional[List[float]]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.

        Thread-safe: PyTorch ëª¨ë¸ ë™ì‹œ ì¶”ë¡  ë°©ì§€ë¥¼ ìœ„í•´ Lock ì‚¬ìš©

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            768ì°¨ì› ì„ë² ë”© ë²¡í„° ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        with self._inference_lock:  # PyTorch ë™ì‹œ ì¶”ë¡  ë°©ì§€
            try:
                # í† í¬ë‚˜ì´ì§•
                encoded_input = self.tokenizer(
                    text,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors='pt'
                )

                # ì„ë² ë”© ìƒì„± (gradient ê³„ì‚° ë¹„í™œì„±í™”)
                with torch.no_grad():
                    logger.debug(f"PyTorch ì¶”ë¡  ì‹œì‘: {len(text)}ì")
                    model_output = self.model(**encoded_input)
                    logger.debug("PyTorch ì¶”ë¡  ì™„ë£Œ")

                # Mean pooling
                embedding = self._mean_pooling(model_output, encoded_input['attention_mask'])

                # ì •ê·œí™” (L2 normalization)
                embedding = torch.nn.functional.normalize(embedding, p=2, dim=1)

                # numpy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
                embedding_list = embedding.cpu().numpy()[0].tolist()

                logger.debug(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embedding_list)}ì°¨ì›")

                return embedding_list

            except Exception as e:
                logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                return None

    def embed_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ì„ë² ë”©í•©ë‹ˆë‹¤.

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (ì‹¤íŒ¨í•œ í•­ëª©ì€ None)
        """
        embeddings = []

        for text in texts:
            embedding = self.embed_text(text)
            embeddings.append(embedding)

            # ë¡œì»¬ ëª¨ë¸ì´ë¯€ë¡œ rate limit ì—†ìŒ (sleep ì œê±°)

        return embeddings

    def _load_faiss_metadata(self) -> List[Dict[str, Any]]:
        """
        FAISS ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì§ì ‘ ë¡œë“œí•©ë‹ˆë‹¤ (ë™ê¸°).

        Returns:
            ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        metadata_path = settings.FAISS_METADATA_PATH

        if not os.path.exists(metadata_path):
            return []

        try:
            with open(metadata_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            logger.warning(f"FAISS ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def _get_indexed_news_ids(self) -> set:
        """
        FAISSì— ì´ë¯¸ ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ëª©ë¡ì„ ì§ì ‘ ì¡°íšŒí•©ë‹ˆë‹¤ (ë™ê¸°).

        Returns:
            ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ì§‘í•©
        """
        metadata = self._load_faiss_metadata()
        return set(meta["news_article_id"] for meta in metadata)

    def get_unembedded_news(self, db: Session, limit: int = 100) -> List[NewsArticle]:
        """
        ì•„ì§ ì„ë² ë”©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜

        Returns:
            ì„ë² ë”©ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # FAISS ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ì¸ë±ì‹±ëœ ë‰´ìŠ¤ ID ì¡°íšŒ (ë™ê¸°)
            embedded_news_ids = self._get_indexed_news_ids()
            logger.info(f"FAISSì— ì´ë¯¸ ì €ì¥ëœ ë‰´ìŠ¤: {len(embedded_news_ids)}ê±´")

        except Exception as e:
            logger.warning(f"FAISS ì¡°íšŒ ì‹¤íŒ¨ (ëª¨ë“  ë‰´ìŠ¤ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì²˜ë¦¬): {e}")
            embedded_news_ids = set()

        # PostgreSQLì—ì„œ ë¯¸ì„ë² ë”© ë‰´ìŠ¤ ì¡°íšŒ
        if embedded_news_ids:
            unembedded_news = (
                db.query(NewsArticle)
                .filter(NewsArticle.id.notin_(embedded_news_ids))
                .order_by(NewsArticle.published_at.desc())
                .limit(limit)
                .all()
            )
        else:
            unembedded_news = (
                db.query(NewsArticle)
                .order_by(NewsArticle.published_at.desc())
                .limit(limit)
                .all()
            )

        logger.info(f"ë¯¸ì„ë² ë”© ë‰´ìŠ¤: {len(unembedded_news)}ê±´")
        return unembedded_news

    def save_to_faiss(
        self, news_list: List[NewsArticle], embeddings: List[List[float]]
    ) -> int:
        """
        ë‰´ìŠ¤ ì„ë² ë”©ì„ FAISSì— ì§ì ‘ ì €ì¥í•©ë‹ˆë‹¤ (ë™ê¸°).

        Args:
            news_list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

        Returns:
            ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
        """
        if len(news_list) != len(embeddings):
            logger.error("ë‰´ìŠ¤ì™€ ì„ë² ë”© ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return 0

        try:
            index_path = settings.FAISS_INDEX_PATH
            metadata_path = settings.FAISS_METADATA_PATH

            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(index_path), exist_ok=True)
            os.makedirs(os.path.dirname(metadata_path), exist_ok=True)

            # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±
            if os.path.exists(index_path):
                index = faiss.read_index(index_path)
            else:
                index = faiss.IndexFlatL2(settings.EMBEDDING_DIM)

            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata = self._load_faiss_metadata()

            # ì„ë² ë”©ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            embeddings_np = np.array(embeddings, dtype=np.float32)

            # FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€
            index.add(embeddings_np)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for news in news_list:
                metadata.append({
                    "news_article_id": news.id,
                    "stock_code": news.stock_code or "",
                    "published_at": int(news.published_at.timestamp()),
                })

            # ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(index, index_path)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            with open(metadata_path, 'wb') as f:
                pickle.dump(metadata, f)

            logger.info(f"ğŸ’¾ FAISSì— {len(news_list)}ê±´ ì €ì¥ ì™„ë£Œ (ì´ {index.ntotal}ê°œ ë²¡í„°)")
            return len(news_list)

        except Exception as e:
            logger.error(f"FAISS ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0

    def embed_and_save_news(
        self, db: Session, batch_size: int = 100
    ) -> Tuple[int, int]:
        """
        ë¯¸ì„ë² ë”© ë‰´ìŠ¤ë¥¼ ì„ë² ë”©í•˜ì—¬ Milvusì— ì €ì¥í•©ë‹ˆë‹¤.

        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜) íŠœí”Œ
        """
        logger.info("=" * 60)
        logger.info("ğŸ”¤ ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—… ì‹œì‘")
        logger.info("=" * 60)

        try:
            # ë¯¸ì„ë² ë”© ë‰´ìŠ¤ ì¡°íšŒ
            unembedded_news = self.get_unembedded_news(db, limit=batch_size)

            if not unembedded_news:
                logger.info("ì„ë² ë”©í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return 0, 0

            logger.info(f"ì„ë² ë”© ëŒ€ìƒ ë‰´ìŠ¤: {len(unembedded_news)}ê±´")

            # í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì œëª© + ë³¸ë¬¸)
            texts = [f"{news.title}\n{news.content}" for news in unembedded_news]

            # ì„ë² ë”© ìƒì„±
            logger.info("ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ìƒì„± ì¤‘...")
            embeddings = self.embed_batch(texts)

            # ì„±ê³µ/ì‹¤íŒ¨ ë¶„ë¥˜
            success_news = []
            success_embeddings = []
            fail_count = 0

            for news, embedding in zip(unembedded_news, embeddings):
                if embedding is not None:
                    success_news.append(news)
                    success_embeddings.append(embedding)
                else:
                    fail_count += 1
                    logger.warning(f"ë‰´ìŠ¤ ID {news.id} ì„ë² ë”© ì‹¤íŒ¨")

            # FAISSì— ì €ì¥
            if success_embeddings:
                saved_count = self.save_to_faiss(success_news, success_embeddings)
                logger.info(
                    f"âœ… ì„ë² ë”© ì™„ë£Œ: ì„±ê³µ {saved_count}ê±´, ì‹¤íŒ¨ {fail_count}ê±´"
                )
                return saved_count, fail_count
            else:
                logger.warning("ì €ì¥í•  ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤")
                return 0, fail_count

        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—… ì¤‘ ì—ëŸ¬: {e}", exc_info=True)
            return 0, 0


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ (Thread-safe)
_news_embedder: Optional[NewsEmbedder] = None
_embedder_lock = threading.Lock()


def get_news_embedder() -> NewsEmbedder:
    """
    NewsEmbedder ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (Thread-safe).

    Double-checked locking íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ
    ì•ˆì „í•˜ê²Œ ì‹±ê¸€í†¤ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Returns:
        NewsEmbedder ì¸ìŠ¤í„´ìŠ¤
    """
    global _news_embedder

    # First check (without lock) - ì„±ëŠ¥ ìµœì í™”
    if _news_embedder is None:
        with _embedder_lock:
            # Second check (with lock) - thread-safety ë³´ì¥
            if _news_embedder is None:
                _news_embedder = NewsEmbedder()

    return _news_embedder


def run_daily_embedding(batch_size: int = 100) -> Tuple[int, int]:
    """
    ì¼ì¼ ë‰´ìŠ¤ ì„ë² ë”© ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

    Args:
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 100)

    Returns:
        (ì„±ê³µ ê±´ìˆ˜, ì‹¤íŒ¨ ê±´ìˆ˜) íŠœí”Œ
    """
    db = SessionLocal()
    embedder = get_news_embedder()

    try:
        return embedder.embed_and_save_news(db, batch_size=batch_size)
    finally:
        db.close()
